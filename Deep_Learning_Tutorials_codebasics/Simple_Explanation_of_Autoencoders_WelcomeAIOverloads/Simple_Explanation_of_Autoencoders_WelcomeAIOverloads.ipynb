{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Explanation of Autoencoders \n",
    "\n",
    "Link to the Youtube tutorial video: https://www.youtube.com/watch?v=3jmcHZq3A5s\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Intuition of the working principle of an autoencoder:**\n",
    "1) In their simplest form, an **autoencoder is a neural network** that attempts to do two things. **First it, compresses its input data into a lower dimension, then it tries to use this lower dimensional representation of the data to recreate the original input. The difference between the attempted recreation and the original input is called the reconstruction error. By training the network to minimize this reconstruction error on your dataset, the network learns to exploit the natural structure in your data to find an efficient lower dimensional representation.**\n",
    "    1) <img src=\"hidden\\photo1.png\" alt=\"This image is a representation of the simple neural network\" style=\"width: 400px;\"/>  <br />\n",
    "2) The left part of the network is called the encoder. Its job is to transform the original input (each neuron in the first layer of the encoder of the autoencoder receives a feature of the original input, similar to the concept of useing ANN to perform image classification) into a lower dimensional representation (represented by a vector (embedding vector/bottleneck) with lower dimension[lower number of elements, where each element is an output of the neuron at the middle layer of the autoencoder (bottleneck)]).\n",
    "    1) <img src=\"hidden\\photo2.png\" alt=\"This image is a representation of the simple neural network\" style=\"width: 400px;\"/>  <br />\n",
    "    2) <img src=\"hidden\\photo3.png\" alt=\"This image is a representation of the simple neural network\" style=\"width: 400px;\"/>  <br />\n",
    "3) Imagine do two inputs, (city,country). Maybe you have (Tokyo,Japan), (Paris,France) and so on. Even though it's conceptually possible to have (Hong Kong, Spain), we don't actually see this in the real data. This is because real data often lies on a lower dimensional subspace within the full dimensionality of the input. The point is the real data isn't fully spread out across all possibilities, but actually makes up a small fraction of the possible space. For instance, here's an example, points that are evenly spread throughout three-dimensional space. There's no structure here because the data is totally random and there's no way to describe the location of all of these points using fewer than three numbers per point, without losing information. Because this data truly spans all three dimensions. In practice, our data has structure which is another way of saying that it's constrained. Remember (Hong Kong, Spain) is conceptually possible but we won't ever see it in the real data, so that part of the space is unoccupied. Here's an example of constrained data. In the same space, we can still describe each point with three numbers. But this is somehow inefficient since the real data is constrained to a one-dimensional spiral. The trick would then be to find a new coordinate system where the constraints of the spiral are ingrained into it and then we would only need a single number to describe any point without information loss. **For this spiral example, we can represent it exactly here are the equations that translate the single angular dimension theta (lower dimensional representation) into the original three dimensions (higher dimensional representation) for any particular point on the spiral (the same input data/samples). I can choose to describe it with a single number theta or I can describe it with three numbers XY and Z. It just depends on the coordinate system I'm using.**\n",
    "    1) <img src=\"hidden\\photo4.png\" alt=\"This image is a representation of the simple neural network\" style=\"width: 400px;\"/>  <br />\n",
    "    2) <img src=\"hidden\\photo5.png\" alt=\"This image is a representation of the simple neural network\" style=\"width: 400px;\"/>  <br />\n",
    "4) The decoder attempts to recreate the original input using the output of the encoder. In other words, it tries to reverse the encoding process. This is interesting because it's trying to recreate a higher dimensional thing using a lower dimensional thing. This is a bit like trying to build a house by looking at a picture.\n",
    "    1) <img src=\"hidden\\photo6.png\" alt=\"This image is a representation of the simple neural network\" style=\"width: 400px;\"/>  <br />\n",
    "    2) <img src=\"hidden\\photo7.png\" alt=\"This image is a representation of the simple neural network\" style=\"width: 400px;\"/>  <br />\n",
    "5) We mentioned before that your true data can likely be described using fewer dimensions than the original input space, but the point of the middle layer in an autoencoder is to make it even smaller than that. This forces information loss, which is key to this whole process. Working by making it so that the decoder has imperfect information.\n",
    "    1) <img src=\"hidden\\photo8.png\" alt=\"This image is a representation of the simple neural network\" style=\"width: 400px;\"/>  <br />\n",
    "6) And training the whole network of the autoencoder to minimize the reconstruction error, we forced the encoder and decoder to work together to find the most efficient way to condense the input data into a lower dimension.\n",
    "    1) <img src=\"hidden\\photo9.png\" alt=\"This image is a representation of the simple neural network\" style=\"width: 400px;\"/>  <br />\n",
    "7) If we did not have information lost between the encoder and decoder, then the network could simply learn to multiply the input by 1 and get a perfect reconstruction. This would obviously be a useless solution. We don't need a fancy neural network just to multiply something by 1. The only way auto-encoders work is by enforcing this information loss, with the network bottleneck. But this means we need to tune the architecture of our network so that the inner dimension is less than the dimension needed to express our data.\n",
    "    1) <img src=\"hidden\\photo10.png\" alt=\"This image is a representation of the simple neural network\" style=\"width: 400px;\"/>  <br />\n",
    "8) But how could you know that in advance what we really want is a way of learning these representations using whatever architecture we want without the fear that the network's gonna learn this trivial solution of multiplying by one? Luckily there's a clever tweak. We can avoid that problem and this gets us into the world of denoising autoencoders. The idea is before you pass the input into the network, you add noise to it. So if it's an image, maybe you add blur then you ask the network to learn how to erase the noise that you just added and reconstruct the original input. So the reconstruction error is slightly modified so that the input to the encoder now has a noise term added. This means the network multiplying the input by 1 is no longer a good solution because this would just return the distorted image and still have a large reconstruction error. This is called a denoising autoencoder, because it attempts to remove the noise that we added artificially to the raw data.\n",
    "    1) <img src=\"hidden\\photo11.png\" alt=\"This image is a representation of the simple neural network\" style=\"width: 400px;\"/>  <br />\n",
    "9) Now that we have an understanding of how autoencoders are structured and learned, let's talk about some ways you can use them. The first is as a feature extractor. In this case, after we complete the training process, we chop off and throw away the decoder and just use the encoder part of the network. The encoder then transforms our raw data into this new coordinate system. And if you visualize the data in this new space (coordinate system), you'll find that similar records are clustered together.\n",
    "    1) <img src=\"hidden\\photo12.png\" alt=\"This image is a representation of the simple neural network\" style=\"width: 400px;\"/>  <br />\n",
    "10) If you don't have any labels at all, you can still use autoencoder for anomaly detection. In this case, you keep the full autoencoder and use the reconstruction error as the anomaly score. To grasp of this, consider our previous example of a one-dimensional spiral in three-dimensional space. What happens if we train our autoencoder on these spiral points with an input of an anomalous random point that's far from the spiral? Since our autoencoder who would have only seen spiral points, the decoder would likely return a point that's close to the spiral even though the input point was far from it. So for anomalous input points that are far from the spiral, we expect a large reconstruction error since the autoencoder just can't represent it well. This is why the reconstruction error can be a proxy for an anomaly score. The nature of an anomaly is that it doesn't respect the normal structure of the data and this is where the autoencoder will have a hard time.\n",
    "    1) <img src=\"hidden\\photo13.png\" alt=\"This image is a representation of the simple neural network\" style=\"width: 400px;\"/>  <br />\n",
    "    2) <img src=\"hidden\\photo14.png\" alt=\"This image is a representation of the simple neural network\" style=\"width: 400px;\"/>  <br />\n",
    "11) You can use denoising autoencoder for missing value imputation. As an example, let's say you have these four rows of data where the first three rows are complete, but the last record has a missing value. The idea is that we train the network (autoencoder) by randomly replacing true data with missing data (noise) and ask it to learn to erase the noise. Then, once the model (autoencoder) is trained, we can pass in inputs that actually have missing fields and use the network (autoencoder) to predict what the missing values are likely to be. We can then use these predictions to replace our actual missing values.\n",
    "    1) <img src=\"hidden\\photo15.png\" alt=\"This image is a representation of the simple neural network\" style=\"width: 400px;\"/>  <br />\n",
    "    2) <img src=\"hidden\\photo16.png\" alt=\"This image is a representation of the simple neural network\" style=\"width: 400px;\"/>  <br />"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
