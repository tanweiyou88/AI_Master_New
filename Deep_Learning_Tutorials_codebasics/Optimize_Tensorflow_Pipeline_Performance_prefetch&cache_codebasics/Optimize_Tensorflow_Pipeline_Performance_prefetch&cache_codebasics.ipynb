{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimize Tensorflow Pipeline Performance: Prefetch & Cache\n",
    "\n",
    "Link to the Youtube tutorial video: https://www.youtube.com/watch?v=MLEKEplgCas&list=PLeo1K3hjS3uu7CxAacxVndI4bE_o3BDtO&index=45\n",
    "\n",
    "To observe the significant outputs in this tutorial, CPU & GPU are required. Must activate the anaconda virtual environment that enables & recognizes GPU (enter: activate GPUEnv) on anaconda prompt before runing this script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a class with tf.data.Dataset as a base class. When you supply \"tf.data.Dataset\" as the argument, the FileDataSet class will be derived from \"tf.data.Dataset\".\n",
    "class FileDataSet(tf.data.Dataset): # In this tutorail, we are measuring the performance (the training time) using prefetch. We will see how using prefetch you can optimize the use of CPU and GPU, and you can get a better training performance. To mimic the real world scenario (EG: latencies in reading files or reading objects from the storage), we are creating this dummy class\n",
    "    def read_files_in_batches(num_samples):\n",
    "        # open file  # Assume in real-life, you have some codes to open the file containing your training dataset\n",
    "        time.sleep(0.03) # Mimic the delay in opening the file in real-life\n",
    "        for sample_index in range(num_samples): # Mimic read the files/samples in your training dataset\n",
    "            time.sleep(0.015) # Mimic the delay in reading each sample in real-life\n",
    "            yield(sample_index,) # yield() return a generator. Detail information of yield(): https://stackoverflow.com/questions/231767/what-does-the-yield-keyword-do-in-python\n",
    "\n",
    "    def __new__(cls, num_samples = 3): # This __new__ function overites the read_files_in_batches, when FileDataSet is called. cls is the class reference. Set the num_samples = 3 as default.\n",
    "        return tf.data.Dataset.from_generator( # Use a generator\n",
    "            cls.read_files_in_batches, # Perform the operations specified in read_files_in_batches\n",
    "            output_signature=tf.TensorSpec(shape=(1,), dtype= tf.int64), # output_signature specifies the format of data the __new__ will return\n",
    "            args = (num_samples,) # The argument (Here, is num_samples) you supply to the function of read_files_in_batches\n",
    "        )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Self-define a benchmark function, to evaluate the training performance (in terms of time). The function takes the dataset and number of epochs (Here, we set the number of epochs = 2 as default) as the inputs.\n",
    "def benchmark(dataset, num_epochs=2): # dataset is actually the FileDataset()\n",
    "    for epoch_num in range(num_epochs): # Go through all epochs\n",
    "        for sample in dataset: # Go through all samples in your dataset\n",
    "            time.sleep(0.01) # Mimic the delay in going through each sample in your dataset in real-life"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Effect of prefetch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmark the performance (training time) of FileDataSet()\n",
    "\n",
    "In FileDataSet(), the CPU fetches a batch of samples, then the GPU performs training on that batch of samples, then the CPU only fetches another batch of samples. These operations are performed sequentially (one-by-one).\n",
    "\n",
    "<img src=\"hidden\\photo1.png\" alt=\"This image is a representation of the simple neural network\" style=\"width: 450px;\"/>  <br />\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "360 ms ± 8.91 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit # Use the line magic of timeit to get the performance (the training time)\n",
    "benchmark(FileDataSet()) # Benchmark the performance (training time) of FileDataSet()\n",
    "\n",
    "# Insights:\n",
    "# 1) Without prefetching, the performance (training time) is 388 ms, the longest duration compared to the ones of prefetch(1) & prefetch(tf.data.AUTOTUNE) in this tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmark the performance (training time) of FileDataSet().prefetch()\n",
    "\n",
    "In FileDataSet().prefetch(), when the training is performed by GPU, at the same time, a new batch of training samples is prefetched by CPU for next training.\n",
    "\n",
    " <img src=\"hidden\\photo2.png\" alt=\"This image is a representation of the simple neural network\" style=\"width: 450px;\"/>  <br />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using prefetch(1), meaning prefetch 1 batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "319 ms ± 16.4 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit # Use the line magic of timeit to get the performance (the training time)\n",
    "benchmark(FileDataSet().prefetch(1)) # Benchmark the performance (training time) of FileDataSet().prefetch(1). .prefetch(1) means to prefetch 1 batch of samples while your GPU is training; .prefetch(tf.data.AUTOTUNE) means to autotune will figure out on its own how many batches it wants to prefetch while your GPU is training. We can use .prefetch() on FileDataset() because FileDataset() has the class of tf.data.Dataset.\n",
    "\n",
    "# Insights:\n",
    "# 1) With prefetch(1), the performance (training time) is 323 ms, the shortest duration compared to the ones of without prefetching & prefetch(tf.data.AUTOTUNE) in this tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using prefetch(tf.data.AUTOTUNE), meaning the number of batches to be prefetched is determined by autotune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "324 ms ± 12.4 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit # Use the line magic of timeit to get the performance (the training time)\n",
    "benchmark(FileDataSet().prefetch(tf.data.AUTOTUNE)) # Benchmark the performance (training time) of FileDataSet().prefetch(1). .prefetch(1) means to prefetch 1 batch of samples; .prefetch(tf.data.AUTOTUNE) means to autotune will figure out on its own how many batches it wants to prefetch. We can use .prefetch() on FileDataset() because FileDataset() has the class of tf.data.Dataset.\n",
    "\n",
    "# Insights:\n",
    "# 1) With prefetch(tf.data.AUTOTUNE), the performance (training time) is 332 ms, the middle duration compared to the ones of prefetch(1) & without prefetching in this tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Effect of cache()\n",
    "\n",
    "1) <img src=\"hidden\\photo3.png\" alt=\"This image is a representation of the simple neural network\" style=\"width: 450px;\"/>  <br />\n",
    "    1) Without cache(), operations of opening the file containing dataset, read the files/samples of the dataset, and perform mapping on the files/samples are repeated for every epoch before training.\n",
    "\n",
    "2) <img src=\"hidden\\photo4.png\" alt=\"This image is a representation of the simple neural network\" style=\"width: 450px;\"/>  <br />\n",
    "    1) With cache(), operations of opening the file containing dataset, read the files/samples of the dataset, and perform mapping on the files/samples only performed 1 time at the first epoch before training. The processed files/images will be stored in the cache of the laptop. At the upcoming epochs (2nd epoch, 3rd epoch, ... ), the processed files/images are retrieved from the cache to perform training (without performing the operations to process the files/samples again). Hence, cache() saves(improves) training times."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use a simple dataset to explain the concept of cache() in a simple way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The samples in the dataset variable:\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "\n",
      "The samples in the dataset_squared variable:\n",
      "0\n",
      "1\n",
      "4\n",
      "9\n",
      "16\n"
     ]
    }
   ],
   "source": [
    "# Create a new dataset, consisting of a bunch of numbers\n",
    "dataset = tf.data.Dataset.range(5)\n",
    "\n",
    "# Show the samples \n",
    "print('The samples in the dataset variable:')\n",
    "for sample in dataset:\n",
    "    print(sample.numpy())\n",
    "\n",
    "# Compute the square of each sample in the dataset\n",
    "dataset = dataset.map(lambda x: x**2)\n",
    "print('\\nThe samples in the dataset_squared variable:')\n",
    "for sample in dataset:\n",
    "    print(sample.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The samples in the dataset_squared variable:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0, 1, 4, 9, 16]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cache the dataset (means store the dataset in cache. So later when we call dataset again, the squared samples of the dataset are retrieved/read from cache of the laptop, but the function map(lambda x: x**2) is not executed again). If without cache, everytime you call dataset in this case, you perform the function map(lambda x: x**2) on samples of dataset before you get the squared samples.\n",
    "dataset = dataset.cache()\n",
    "print('\\nThe samples in the dataset_squared variable:')\n",
    "list(dataset.as_numpy_iterator()) # The alternative method to print the elements in a variable (similar to: for sample in dataset_squared: print(sample.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use the FileDataSet dataset to explain the concept of cache() in a practical way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Self-define a function to provide delay\n",
    "def mapped_function(s): # Here, s is just dummy variable which we ignore.\n",
    "    tf.py_function(lambda: time.sleep(0.03), [], ()) # This delay is the only thing we want in this function.\n",
    "    return s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Benchmark the performance of training without cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.52 s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit -n1 -r1 # %%time is a 'cell magic' and has to be the first thing in the IPython (Jupyter) cell. I can reproduce this error if for example I have a comment first. When %%time is not the first thing in the cell, IPython tries to interpret it as a 'line magic' hence the error you see.\n",
    "benchmark(FileDataSet().map(mapped_function), 5) # 5 means 5 epochs\n",
    "\n",
    "# Insights:\n",
    "# 1) Without cache(), the performance (training time) is 1.52 s, the longest duration compared to the ones of with cache() in this tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Benchmark the performance of training with cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "567 ms ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit -n1 -r1\n",
    "benchmark(FileDataSet().map(mapped_function).cache(), 5) # 5 means 5 epochs\n",
    "\n",
    "# Insights:\n",
    "# 1) With cache(), the performance (training time) is 567 ms, the shortest duration compared to the ones of without cache() in this tutorial\n",
    "# 2) Because what would cache() have done is, see I'm running it for 5 epochs. At the first epoch, when I call mapped_function, it will introduce a delay. But at the second time, the data is cached. So at the second time, on our second, third, fourth, and fifth epoch, it is not calling this mapped_function. It is using the map data from the cache itself."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
