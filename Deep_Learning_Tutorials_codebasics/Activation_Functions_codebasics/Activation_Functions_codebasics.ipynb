{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Activation Functions Introduction\n",
    "\n",
    "Link to the Youtube tutorial video: https://www.youtube.com/watch?v=icZItWxw7AI&list=PLeo1K3hjS3uu7CxAacxVndI4bE_o3BDtO&index=8&t=2s\n",
    "\n",
    "**A single neuron neural network:**  <br />\n",
    "1) The neuron has 2 portions: weighted sum of the input; activation function\n",
    "2) Having a sigmoid function as the activation function helps you to reduce the output into the range between 0 and 1, so that you can make a decision for your classification. <br />\n",
    "3) If you have value between negative infinity and positive infinity, it becomes hard to make that decision.  <br />\n",
    "4) For this reason, the sigmoid function is called activation function which means it will decide whether your neuron is firing or not firing. Firing means the output is TRUE (EG: the person will buy the insurance). Not firing means the output is FALSE (EG: the person will NOT buy the insurance).  <br />\n",
    "5) Hence, activation function is helpful in the output layer.  <br />\n",
    "<img src=\"hidden\\photo1.png\" alt=\"This image is a representation of the simple neural network\" style=\"width: 400px;\"/>  <br /><br />\n",
    "\n",
    "**A neural network with hidden layer:**  <br />\n",
    "1) Each neuron in a hidden layer has 2 portions: weighted sum of the input; activation function\n",
    "2) If you remove the activation function of all neurons in a hidden layer, you will eventually get a linear equation as the output which is just a weighted sum of your input features (for this reason, you do not even need hidden layer if you remove activation function). <br />\n",
    "3) Since the real life problems are non-linear (EG: the patterns we see in the universe cannot be expressed by linear equation), we cannot solve the problems using linear equation. Hence, you need non-linear equation all  the time. <br />\n",
    "4) Activation function will help you to build the non-linear equation.  <br />\n",
    "<img src=\"hidden\\photo2.png\" alt=\"This image is a representation of the simple neural network\" style=\"width: 400px;\"/>  <br />  <br />\n",
    "\n",
    "**Step function:**  <br />\n",
    "1) Step function is a type of activation function.  <br />\n",
    "2) The first problem of step function is it misclassifies some data points.  <br />\n",
    "3) The second problem of step function is when you are doing multi-class classification (EG: Output class ranged from '1' to '9'), the step function will only give 2 output (EG:0 or 1) and you might end up getting one or more classes as TRUE and you hard to make a decision (don't know what is your final class).  <br />\n",
    "<img src=\"hidden\\photo3.png\" alt=\"This image is a representation of the simple neural network\" style=\"width: 400px;\"/>  <br />\n",
    "<img src=\"hidden\\photo4.png\" alt=\"This image is a representation of the simple neural network\" style=\"width: 400px;\"/>  <br />  <br />\n",
    "\n",
    "**Sigmoid function:**  <br />\n",
    "1) Sigmoid function solves the second problem of step function mentioned above.  <br />\n",
    "2) Sigmoid function gives you a smooth curve between 0 and 1. And because of this when you are doing multi-class classification, you have a number between 0 and 1 and now you can get the output class with maximum value/score easily.  <br />\n",
    "<img src=\"hidden\\photo5.png\" alt=\"This image is a representation of the simple neural network\" style=\"width: 400px;\"/>  <br />  <br />\n",
    "\n",
    "**Tanh function:**  <br />\n",
    "1) Tanh function is similar to sigmoid function, but it gives output between -1 and 1.   <br />\n",
    "<img src=\"hidden\\photo6.png\" alt=\"This image is a representation of the simple neural network\" style=\"width: 400px;\"/>  <br />  <br />\n",
    "\n",
    "**Guidelines:**  <br />\n",
    "1) <img src=\"hidden\\photo7.png\" alt=\"This image is a representation of the simple neural network\" style=\"width: 400px;\"/>  <br />\n",
    "    1) Because Tanh function will kind of calculate a mean of 0 and it will center your data.\n",
    "2) <img src=\"hidden\\photo8.png\" alt=\"This image is a representation of the simple neural network\" style=\"width: 400px;\"/>  <br />\n",
    "    1) However, the issue with Tanh and sigmoid functions is when the value (feature) is higher, you get your derivative as 0, and this creates a problem in your learning process because for gradient descent concept, you need to calculate the derivative and backpropogate your errors. And if your derivatives are closing to 0, the learning becomes extremely slow. This is called vanishing gradients problem. In short, the sigmoid and Tanh functions has vanishing gradient problem and this problem makes the learning process very slow.  <br />\n",
    "3) <img src=\"hidden\\photo9.png\" alt=\"This image is a representation of the simple neural network\" style=\"width: 400px;\"/>  <br />\n",
    "    1) To solve the vanishing gradient problem of Tanh and sigmoid functions, ReLU function is introduced. ReLU function is an extremely simple function that if your input value is less than 0, then your output value is 0; if your input value is at least 0, then your output value equals to the input value.  <br />\n",
    "    2) ReLU function also has vanishing gradient problem because if the input value is less than 0, the derivative again is 0.  <br />\n",
    "4) <img src=\"hidden\\photo11.png\" alt=\"This image is a representation of the simple neural network\" style=\"width: 400px;\"/>  <br /> \n",
    "    1) To solve the vanishing gradient problem of ReLU function, the leaky ReLU function is introduced.  <br />\n",
    "    2) For the region where the input value is less than 0, the leaky ReLU function will try to reduce the value close to 0, and there is still some sort of linear line.  <br />\n",
    "5) <img src=\"hidden\\photo10.png\" alt=\"This image is a representation of the simple neural network\" style=\"width: 400px;\"/>  <br />\n",
    "    1) Because in terms of math, the sigmoind and Tanh functions are computationally complex. However, ReLU function is very computationally effective and lightweight.   <br />  <br />\n",
    "\n",
    "**Summary of activation functions:**  <br />\n",
    "1) Sometimes there is no clear answer which activation function you should use for your problem. The things of neural network or machine learning are all about trial and error, so you have to try different activation functions and see which activation functions gives you the best output.  <br />\n",
    "2) If you have binary classification, you will probably use sigmoid function.  <br />\n",
    "3) In the hidden layer, you will probably use ReLU or leaky ReLU functions.  <br />\n",
    "<img src=\"hidden\\photo12.png\" alt=\"This image is a representation of the simple neural network\" style=\"width: 400px;\"/>  <br />\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the activation function called: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sigmoid function\n",
    "\n",
    "Sigmoid function will just try to convert any value into a range of 0 to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For x = 100, sigmoid(x) =  1.0\n",
      "For x = 1, sigmoid(x) =  0.7310585786300049\n",
      "For x = 0.5, sigmoid(x) =  0.6224593312018546\n",
      "For x = -56, sigmoid(x) =  4.780892883885469e-25\n"
     ]
    }
   ],
   "source": [
    "# Self-define the sigmoid function using python code\n",
    "def sigmoid(x):\n",
    "    return 1/(1 + math.exp(-x))\n",
    "\n",
    "print('For x = 100, sigmoid(x) = ', sigmoid(100))\n",
    "print('For x = 1, sigmoid(x) = ', sigmoid(1))\n",
    "print('For x = 0.5, sigmoid(x) = ', sigmoid(0.5))\n",
    "print('For x = -56, sigmoid(x) = ', sigmoid(-56))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tanh function\n",
    "\n",
    "Tanh function is a variant of the sigmoid function that will just try to convert any value into a range of -1 to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For x = 50, tanh(x) =  1.0\n",
      "For x = 1, tanh(x) =  0.7615941559557649\n",
      "For x = -56, tanh(x) =  -1.0\n"
     ]
    }
   ],
   "source": [
    "# Self-define the Tanh function using python code\n",
    "def tanh(x):\n",
    "    return ( (math.exp(x) - math.exp(-x)) / (math.exp(x) + math.exp(-x)))\n",
    "\n",
    "print('For x = 50, tanh(x) = ', tanh(50))\n",
    "print('For x = 1, tanh(x) = ', tanh(1))\n",
    "print('For x = -56, tanh(x) = ', tanh(-56))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ReLU function\n",
    "\n",
    "1) ReLU function will just take/return/output the maximum value between 0 and the input value.\n",
    "2) In other words, if the input value is a:\n",
    "    1) negative value, ReLU function will return 0\n",
    "    1) positive value, ReLU function will return the same input value (the input value is remained at the output of the ReLU function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For x = 100, relu(x) =  100\n",
      "For x = 25, relu(x) =  25\n",
      "For x = -10, relu(x) =  0\n",
      "For x = -0.8, relu(x) =  0\n"
     ]
    }
   ],
   "source": [
    "# Self-define the ReLU function using python code\n",
    "def relu(x):\n",
    "        return max(0,x)\n",
    "\n",
    "print('For x = 100, relu(x) = ', relu(100))\n",
    "print('For x = 25, relu(x) = ', relu(25))\n",
    "print('For x = -10, relu(x) = ', relu(-10))\n",
    "print('For x = -0.8, relu(x) = ', relu(-0.8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Leaky ReLU function\n",
    "\n",
    "1) If the input value is a:\n",
    "    1) negative value, leaky ReLU function will return a very small negative value\n",
    "    2) positive value, leaky ReLU function will return the same input value (the input value is remained at the output of the ReLU function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For x = 100, leaky_relu(x) =  100\n",
      "For x = 25, leaky_relu(x) =  25\n",
      "For x = -10, leaky_relu(x) =  -1.0\n",
      "For x = -0.8, leaky_relu(x) =  -0.08000000000000002\n"
     ]
    }
   ],
   "source": [
    "# Self-define the leaky ReLU function using python code\n",
    "def leaky_relu(x):\n",
    "    return max(0.1*x,x)\n",
    "\n",
    "print('For x = 100, leaky_relu(x) = ', leaky_relu(100))\n",
    "print('For x = 25, leaky_relu(x) = ', leaky_relu(25))\n",
    "print('For x = -10, leaky_relu(x) = ', leaky_relu(-10))\n",
    "print('For x = -0.8, leaky_relu(x) = ', leaky_relu(-0.8))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
