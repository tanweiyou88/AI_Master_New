{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss or Cost Function\n",
    "\n",
    "Link to the Youtube tutorial video: https://www.youtube.com/watch?v=E1yyaLRUnLo&list=PLeo1K3hjS3uu7CxAacxVndI4bE_o3BDtO&index=11\n",
    "\n",
    "\n",
    "1) In general, in machine learning, having squared error allows your gradient descent to converge in a better way.  <br />\n",
    "2) Loss is heavily used during neural network training.  <br />\n",
    "3) When you feed your neural network with the features belongs to a sample of your train set, it is called a forward pass.   <br />\n",
    "4) Individual error (the difference between the prediction and ground truth of a sample) is called a loss, the cumulative/sum of error is called a cost function.  <br />\n",
    "5) If you goes through (forward pass) all the samples in the train set in 1 round (once), it is called 1 epoch.  <br />\n",
    "6) Practically, for logistic regression, we use binary cross entropy (log loss) as the cost function.   <br />\n",
    "\n",
    "\n",
    "**Important illustrations:**  </br>\n",
    "<img src=\"hidden\\photo1.png\" alt=\"This image is a representation of the simple neural network\" style=\"width: 400px;\"/>  <br />  <br />\n",
    "<img src=\"hidden\\photo2.png\" alt=\"This image is a representation of the simple neural network\" style=\"width: 400px;\"/>  <br />  <br />\n",
    "<img src=\"hidden\\photo3.png\" alt=\"This image is a representation of the simple neural network\" style=\"width: 400px;\"/>  <br />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The numpy array (matrix) called Y_predicted:\n",
      " [1 1 0 0 1]\n",
      "\n",
      "The numpy array (matrix) called Y_true:\n",
      " [0.3 0.7 1.  0.  0.5]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Create a numpy array (matrix) called Y_predicted, which consists of predicted values\n",
    "Y_predicted = np.array([1 ,1 ,0 ,0 ,1])\n",
    "print('The numpy array (matrix) called Y_predicted:\\n', Y_predicted)\n",
    "\n",
    "# Create a numpy array (matrix) called Y_true, which consists of ground truth values\n",
    "Y_true = np.array([0.30, 0.7, 1, 0, 0.5])\n",
    "print('\\nThe numpy array (matrix) called Y_true:\\n', Y_true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cost function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mean Absolute Error (MAE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate mean absolute error using self-defined function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total error:  2.5\n",
      "MAE:  0.5\n",
      "\n",
      "Results of MAE:  0.5\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "1) zip() allows for parallel iteration across multiple iterables. \n",
    "It takes in any number of iterables and returns an iterator that aggregates elements based on the iterables passed.\n",
    "'''\n",
    "\n",
    "# Self-define mean absolute error (MAE)\n",
    "def mae (y_true, y_predicted):\n",
    "    total_error = 0\n",
    "    for yt, yp in zip(y_true, y_predicted): # For each iteration, yt will get 1 element of y_true and yp will get 1 element of y_predicted\n",
    "       total_error += abs(yt - yp) # Calculate the absolute difference of each element, then add it to the variable total_error\n",
    "    print('Total error: ', total_error)\n",
    "    MAE = total_error / len(y_true) # Usually, the number of elements (len) in a variable equals to the number of samples in the variable\n",
    "    print('MAE: ', MAE)  \n",
    "    return MAE # means after executing this line, the program will jump out (exit) from this self-defined block and assign the MAE value to the variable which call this self-define function in the main\n",
    "\n",
    "mae_results = mae(Y_true, Y_predicted)\n",
    "\n",
    "print('\\nResults of MAE: ', mae_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate mean absolute error using functions from numpy module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The absolute difference between each element in Y_predicted and Y_true:\n",
      " [0.7 0.3 1.  0.  0.5]\n",
      "\n",
      "The mean absolute error:  0.5\n"
     ]
    }
   ],
   "source": [
    "# Calculate the absolute difference between each element in Y_predicted and Y_true, using abs() from numpy module\n",
    "print('The absolute difference between each element in Y_predicted and Y_true:\\n', np.abs(Y_predicted - Y_true))\n",
    "\n",
    "# Calculate the mean absolute error for the absolute difference, using mean() from numpy module\n",
    "print('\\nThe mean absolute error: ', np.mean(np.abs(Y_predicted - Y_true)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binary cross entropy\n",
    "\n",
    "Since log(0) is undefined(having infinite value), according to the formula of binary cross entropy, we need to replace all predicted values which are 1 with a value close to 1 & 0 with a value close to 0, so that the binary cross entropy will not provide error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log(Y_predicted_new)=\n",
      " [  0.   0. -inf -inf   0.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\weiyo\\AppData\\Local\\Temp\\ipykernel_6188\\584237867.py:1: RuntimeWarning: divide by zero encountered in log\n",
      "  print('log(Y_predicted_new)=\\n', np.log(Y_predicted))\n"
     ]
    }
   ],
   "source": [
    "print('log(Y_predicted_new)=\\n', np.log(Y_predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Process the predicted values (particularly replacing the predicted value of 0 and 1 only) to avoid log(0) occurs in binary cross entropy formula"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed predicted values:  [0.999999999999999, 0.999999999999999, 1e-15, 1e-15, 0.999999999999999]\n"
     ]
    }
   ],
   "source": [
    "epsilon = 1e-15\n",
    "\n",
    "# Replace the predicted values which are 0 with a value close to 0, using list comprehension\n",
    "Y_predicted_new = [max(i, epsilon) for i in Y_predicted]\n",
    "\n",
    "# Replace the predicted values which are 1 with a value close to 1, using list comprehension\n",
    "Y_predicted_new = [min(i, 1-epsilon) for i in Y_predicted_new]\n",
    "\n",
    "print('Processed predicted values: ', Y_predicted_new)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log(Y_predicted_new)=\n",
      " [-9.99200722e-16 -9.99200722e-16 -3.45387764e+01 -3.45387764e+01\n",
      " -9.99200722e-16]\n",
      "\n",
      "Binary cross entropy (log loss)=  17.2696280766844\n"
     ]
    }
   ],
   "source": [
    "# Convert the Y_predicted_new from the python list (format) [because it was achieved using list comprehension previously] into numpy array (format)\n",
    "Y_predicted_new = np.array(Y_predicted_new)\n",
    "\n",
    "# Calculate the log of each predicted value\n",
    "print('log(Y_predicted_new)=\\n', np.log(Y_predicted_new))\n",
    "\n",
    "# Calculate the binary cross entropy:\n",
    "print('\\nBinary cross entropy (log loss)= ', -np.mean(Y_true*np.log(Y_predicted_new)+(1-Y_true)*np.log(1-Y_predicted_new)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile the codes to calculate binary cross entropy using functions from numpy module into a self-defined function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Binary cross entropy (log loss) =  17.2696280766844\n"
     ]
    }
   ],
   "source": [
    "def log_loss(y_true, y_predicted):\n",
    "    epsilon = 1e-15\n",
    "    y_predicted_new = [max(i, epsilon) for i in y_predicted]\n",
    "    y_predicted_new = [min(i, 1-epsilon) for i in y_predicted_new]\n",
    "    y_predicted_new = np.array(y_predicted_new)\n",
    "    return -np.mean(Y_true*np.log(Y_predicted_new)+(1-Y_true)*np.log(1-Y_predicted_new))\n",
    "\n",
    "print('Binary cross entropy (log loss) = ', log_loss(Y_true, Y_predicted))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
