{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implement Deep Q-Learning with PyTorch and Train Flappy Bird (DQN PyTorch Beginners Tutorial 1, by Johny Code)\n",
    "\n",
    "Link to the Youtube video tutorial: https://www.youtube.com/watch?v=arR7KzlYs4w&list=PL58zEckBH8fCMIVzQCRSZVPUp3ZAVagWi&index=2 <br />\n",
    "Link to the Gymnasium official website: https://gymnasium.farama.org/ <br />\n",
    "Link to the Flappy Bird environment GitHub repository: https://github.com/markub3327/flappy-bird-gymnasium <br />\n",
    "\n",
    "**Instructions to conduct this tutorial:**\n",
    "1) Must use the virtual environment called \"dqnenv\", because this is the only environment installed with Flappy Bird environment on this device.\n",
    "2) Must run agent.py. agent.ipynb is only used to provide explanation in an arranged way, but it cannot shows the game on screen.\n",
    "3) On agent.py, can press F5 to conduct \"Run and debug\", add breakpoint to certain line, and use the debug console (by providing the name of the variables you interested), to observe the information stored in different variables at different instances (at different lines and different iteration)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the flappy bird environment, which is compatible with gymnasium\n",
    "import flappy_bird_gymnasium \n",
    "\n",
    "# Import the gymnasium, which is an API standard for reinforcement learning with a diverse collection of reference environments\n",
    "import gymnasium "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create the environment\n",
    "\n",
    "1) <img src=\"hidden\\photo7.png\" alt=\"This image is a representation of the simple neural network\" style=\"width: 400px;\"/>  <br />\n",
    "    1) Create an instance of the flappy bird environment called env, using the \"FlappyBird-v0\" environment model and with the help of gymnasium. \n",
    "    2) The parameter \"render_mode = \"human\"\" is used to render the game on the screen. \n",
    "    3) The parameter \"use_lidar = False\" is the custom parameter that allow the user to turn on (use the information of) the LIDAR sensor or off (don't use the information of LIDAR) ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of the flappy bird environment called env, using the \"FlappyBird-v0\" environment model and with the help of gymnasium. The parameter \"render_mode=\"human\"\" is used to render the game on the screen. The parameter \"use_lidar=False\" is the custom parameter that allow the user to turn on (use the information of) the LIDAR sensor or off (don't use the information of LIDAR).\n",
    "env = gymnasium.make(\"FlappyBird-v0\", render_mode = \"human\", use_lidar = False) # This means env is the environment that we create here.\n",
    "## Just for extra illustration of different environment, can ignore/comment the line below.\n",
    "# env = gymnasium.make(\"CartPole-v1\", render_mode=\"human\") # This means env is the environment that we create here. \n",
    "\n",
    "obs, _ = env.reset() # call the reset() to initialize the environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agent interact with the environment through observation,reward and action, through a loop block\n",
    "\n",
    "1) **Detail explanation of certain codes in the loop block:**\n",
    "    1) <img src=\"hidden\\photo2.png\" alt=\"This image is a representation of the simple neural network\" style=\"width: 400px;\"/>  <br />\n",
    "        1) sample() will randomly select a value from a given set of values.\n",
    "        2) The action_space of an environment refers to all possible actions that can be taken by the agent. In this environment, the action_space consists of only 2 values(0:the agent[flappy bird] do nothing; 1:the agent[flappy bird] flaps its wing to fly up). The action space of this environment is as below (according to the Flappy Bird environment GitHub repository):\n",
    "            1) <img src=\"hidden\\photo3.png\" alt=\"This image is a representation of the simple neural network\" style=\"width: 200px;\"/>  <br />\n",
    "        3) Hence, the sample() here will only return either 0 or 1. While the returned value is stored in the action variable.\n",
    "    2) <img src=\"hidden\\photo4.png\" alt=\"This image is a representation of the simple neural network\" style=\"width: 400px;\"/>  <br />\n",
    "        1) The step() will give us back the:\n",
    "            1) observation (obs) [means what the next state is], \n",
    "            2) reward [means how much reward we got from the last action], \n",
    "            3) terminated [terminated=True -> if the bird hits the ground or one of the pipes; else, terminated=False], \n",
    "            4) _ means the parameter is not used,\n",
    "            5) info [just contains the additional information you can use for debugging or something]\n",
    "        2) obs\n",
    "            1) The obs is a vector that contains 12 elements, while each element represents a state information as a value (according to the repository, there are 12 different state information considered as observation). The 12 elements represent different state information respectively in sequence, according to the repository of the environemnt. Hence, the first element of the obs vector represents the state information of the last pipe's horizontal position,...., the last element of the obs vector represents the state information of the player's rotation.\n",
    "            2) The observation space of this environment is as below, only refers to Option 2 because we set LIDAR=False[not using LIDAR information] (according to the Flappy Bird environment GitHub repository):\n",
    "                1) <img src=\"hidden\\photo5.png\" alt=\"This image is a representation of the simple neural network\" style=\"width: 500px;\"/>  <br />\n",
    "            3) The elements' value of the obs vector are normalized (transformed into) to the range between -1 and 1 (because the author used this range to train the neural network)\n",
    "        3) reward\n",
    "            1) The reward space of this environment is as below (according to the Flappy Bird environment GitHub repository):\n",
    "                1) <img src=\"hidden\\photo6.png\" alt=\"This image is a representation of the simple neural network\" style=\"width: 200px;\"/>  <br />\n",
    "\n",
    "  <br />\n",
    "  \n",
    "2) **The information obtained at the first iteration of the loop, through debug console:**\n",
    "    1) <img src=\"hidden\\photo1.png\" alt=\"This image is a representation of the simple neural network\" style=\"width: 400px;\"/>  <br />\n",
    "        1) In the first iteration: \n",
    "            1) The action selected is 0, means the flappy bird do nothing.\n",
    "            2) After the action is executed, the observation of 12 different state information is stored in obs vector as image above. The elements' value of the obs vector are normalized (transformed into) to the range between -1 and 1.\n",
    "            3) The env.observation_space shows that each element value is normalized to the range between -1 and 1, while the size of the observation space is 12 (that's why the obs vector consists of 12 elements). Observation space refers to all the state information gathered in an iteration (when the agent arrives at next state after executing the action in existing iteration/state)\n",
    "            4) The reward the agent gained when it arrives at the next state after executing the action in existing iteration/state is 0.1. According to the Flappy Bird environment GitHub repository, the reward of 0.1 means the agent (flappy bird) stays alive when it arrives at the next state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "while True: # This is an infinite loop\n",
    "    # Next action:\n",
    "    # (feed the observation to your agent here)\n",
    "    action = env.action_space.sample() # use sample() on action_space of the environment to get a random action for the agent (flappy bird). \n",
    "\n",
    "    # Processing:\n",
    "    obs, reward, terminated, _, info = env.step(action) # The action selected is passed into the step() to execute that action.\n",
    " \n",
    "    # Checking if the player is still alive\n",
    "    if terminated: # if terminated=True\n",
    "        break # we exit this infinite loop\n",
    "\n",
    "env.close() # close the environment"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dqnenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
