{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q-Learning Algorithm Basic Concept (by Mahesh Huddar)\n",
    "\n",
    "Link to the Youtube video tutorial: https://www.youtube.com/watch?v=J3qX50yyiU0 <br />\n",
    "Detail information of reinforcement learning: https://www.datacamp.com/tutorial/reinforcement-learning-python-introduction?utm_source=google&utm_medium=paid_search&utm_campaignid=19589720824&utm_adgroupid=157156376071&utm_device=m&utm_keyword=&utm_matchtype=&utm_network=g&utm_adpostion=&utm_creative=684592140425&utm_targetid=dsa-2218886984380&utm_loc_interest_ms=&utm_loc_physical_ms=9072887&utm_content=&utm_campaign=230119_1-sea~dsa~tofu_2-b2c_3-row-p2_4-prc_5-na_6-na_7-le_8-pdsh-go_9-nb-e_10-na_11-na&gad_source=1&gclid=Cj0KCQjw5ea1BhC6ARIsAEOG5pzgbLGTr1XjRYxA6OC-QxJNEr-pQl--9lspGApkBDjEiQwRjp71h-IaAjGUEALw_wcB  <br />\n",
    "\n",
    "1) **Basic information:**\n",
    "    1) An agent is the object that interacts with the environment.\n",
    "    2) A state is the location the agent located in the environment.\n",
    "    3) An agent needs to take an action to move from the current state into next state. The agent receives an instant reward (a value) when it takes an action to move from the current state into the next state.\n",
    "    4) State space refers to all the possible locations available in the environment.\n",
    "    5) Action space refers to all the possible actions the agent can take in the environment.\n",
    "    6) An epsiode starts with the agent located at an inital state, followed by the agent takes actions to move into new state (1 iteration of the episode is completed when the agent takes an action to move from the current state into the new state), and ends with either the agent reaches the goal state / unallowed (challenge) state / time-out (the duration the agent interacts (moves) with the environment reaches the maximum time limit). Hence, 1 episode can have multiple iterations.\n",
    "    7) Reward matrix is the matrix whose value represents the instant reward the agent receives if the agent takes an action to move from the current state into the new state.\n",
    "        1) <img src=\"hidden\\photo1.png\" alt=\"This image is a representation of the simple neural network\" style=\"width: 400px;\"/>  <br />\n",
    "        2) With the image above as an example, state 0 to 5 represent the locations (we can treat them as the rooms) in the environment respectively. Action here refers to the action the agent takes from any current state to move into a particular new state. For example, action 2 means regardless the current state of the agent, the agent will take the action to move into state 2 as the next state. R(state,action) = R(1,5) = 100 means the agent receives an instant reward of 100 if the agent located at the current state of state 1 and take the action to move into the state 5 as the next state. R(state,action) = -1 means the agent cannot take that action when its current state is that state, so we ignore all the state-action pairs whose R(state,action) = -1.\n",
    "\n",
    "2) **Information of Q-Learning Algorithm**\n",
    "    1) Q-value is the expected value of reward (the instant reward received by the agent when it takes the action to move from the current state into the next state + the instant reward received by the agent when it takes the best action to move from the next state into the next next state [this part can be discounted by a discount factor called gamma]). Hence, Q-value is a kind of reward that considers 2 actions taken by the agent in sequence. This concept can be understood easily by considering the case when the environment is reset and Q-table is just intialized at the beginning of reinforcement learning (at the first episode).\n",
    "    2) Q-table is initialized (set the entries of the Q-table as 0) only once during the reinforcement learning. The Q-values (the entries of the Q-table) will be updated with multiple episodes of the reinforcement learning.\n",
    "    3) Q-Learning algorithm is a loop function that used to update the Q-values of the Q-table, by conducting multiple episodes. The pseudocode of Q-learning algorithm is as below:\n",
    "        1) <img src=\"hidden\\photo7.png\" alt=\"This image is a representation of the simple neural network\" style=\"width: 400px;\"/>  <br />\n",
    "    3) The example of performing the Q-learning algorithm is as below:\n",
    "        1) <img src=\"hidden\\photo2.png\" alt=\"This image is a representation of the simple neural network\" style=\"width: 400px;\"/>  <br />\n",
    "        2) <img src=\"hidden\\photo3.png\" alt=\"This image is a representation of the simple neural network\" style=\"width: 400px;\"/>  <br />\n",
    "        3) <img src=\"hidden\\photo4.png\" alt=\"This image is a representation of the simple neural network\" style=\"width: 400px;\"/>  <br />\n",
    "        4) <img src=\"hidden\\photo5.png\" alt=\"This image is a representation of the simple neural network\" style=\"width: 400px;\"/>  <br />\n",
    "        5) <img src=\"hidden\\photo6.png\" alt=\"This image is a representation of the simple neural network\" style=\"width: 400px;\"/>  <br />\n",
    "        5) <img src=\"hidden\\photo8.png\" alt=\"This image is a representation of the simple neural network\" style=\"width: 400px;\"/>  <br />\n",
    "        5) <img src=\"hidden\\photo9.png\" alt=\"This image is a representation of the simple neural network\" style=\"width: 400px;\"/>  <br />\n",
    "        5) <img src=\"hidden\\photo10.png\" alt=\"This image is a representation of the simple neural network\" style=\"width: 400px;\"/>  <br />\n",
    "            1) From the image above, we can interpret the Q-values as below:\n",
    "            2) Given the current state, an agent will take the action that has the highest Q-value. \n",
    "                1) If the current state of the agent is state 3, the highest Q-value among the row of state 3 is 80 ( Q(state,action) = Q(3,1) = Q(3,4) = 80). Hence, the agent can either take action 1 (to move from state 3 into state 1) or 4 (to move from state 3 into state 4). After the agent took an action to move from current state of state 3 into the next state, no matter the next state is either state 1 or 4, the agent can take an action to move from the next state into the state 5 (goal state) as the next next state, followed by the episode is terminated. Q(3,2) is not the highest Q-value at the row of state 3 because if the agent takes the action that equals Q(3,2) [means the agent takes the action to move from the state 3 as the current state into the state 2 as the next state], the agent cannot move into the state 5 (goal state) with 1 more step/iteration of the episode only. Instead, the agent requires to take more actions (requring more steps/iterations) to move into the state 5 (goal state). This also shows that the Q-value of a state is higher if the state is regularly followed by other states (EG: next next state) that yield high rewards. \n",
    "                2) However, if the current state of the agent is state 2, the agent will still take the action to move into state 3 as the next state (because given the current state, the agent will take the action that has the highest Q-value), followed by the process mentioned in 2.3.5.2.1 above.\n",
    "                3) Hence, the Q-table is the policy (if-else rules) that tells the agent to take what action, given the current state of the agent is known. No matter which state the agent is located at as the current state, the agent will refer to the Q-table to decide which action to take to move into the next state. Anologously, agent is a human, Q-table is the human's brain. Brain (Q-table) tells human (agent) what is the best action to take (the highest Q-value at the given current state) when the human is at the current state. Just like no matter which state is the current state of the human (agent), his brain (Q-table) will tell him (agent) the best action to take (the highest Q-value at the given current state) to move into the next state. And human (agent) can learn to achieve his goal (move into the goal state and the episode is terminated) better by repeating the tasks for many times (conduct the reinforcement learning algorithm to update Q-table with many episodes). \n",
    "\n",
    "\n",
    "3) **Reward vs Q-value**\n",
    "    1) <img src=\"hidden\\photo11.png\" alt=\"This image is a representation of the simple neural network\" style=\"width: 400px;\"/>  <br />\n",
    "    2) Credit: https://ai.stackexchange.com/questions/19986/what-is-the-relationship-between-the-reward-function-and-the-value-function#:~:text=Whereas%20rewards%20determine%20the%20immediate%2C%20intrinsic%20desirability%20of,follow%20and%20the%20rewards%20available%20in%20those%20states.\n",
    "    3) A reward refers to the instant reward the agent receives when it takes an action to move from the current state into the next state (so reward considering the total reward the agent receives for only taking 1 action in one-shot). A Q-value refers to the total reward the agent receives when it takes 2 actions to move into 2 new states (take 1 action to move into 1 new state at a time in sequence, so Q-value considering the total reward the agent receives for taking 2 actions in one-shot).\n",
    "    4) Q-value taking into the consideration of the discount factor (gamma). Mathematically, the multiplication of the discount gamma (a value less than 0) makes its output smaller (the more frequent the multiplication of a number with a value less than 0, the smaller the output of the multiplication output is). This discount gamma introduces the concept of economic where the RM1 today is more valuable than the RM1 in future. Analogously, this discount gamma introduces the concept of the faster (the smallest iterations of the episode = the shortest path) the agent moves into the goal state, the better it is, through the phenomenon that the more the iterations taken by the agent in an episode, the less the cumulative reward (the sum of reward the agent receives by taking all the actions in sequence in that episode) received by that agent in that episode.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
