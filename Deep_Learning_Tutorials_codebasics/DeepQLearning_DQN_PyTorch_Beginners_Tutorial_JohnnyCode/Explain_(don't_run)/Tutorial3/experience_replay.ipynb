{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implement Experience Replay (DQN PyTorch Beginners Tutorial 3, by Johny Code)\n",
    "\n",
    "Link to the Youtube video tutorial:  https://www.youtube.com/watch?v=y3BSPfmMIkA&list=PL58zEckBH8fCMIVzQCRSZVPUp3ZAVagWi&index=3   <br />\n",
    "Link to the Gymnasium official website: https://gymnasium.farama.org/ <br />\n",
    "Link to the Flappy Bird environment GitHub repository: https://github.com/markub3327/flappy-bird-gymnasium <br />\n",
    "\n",
    "**Information of Experience Replay:**\n",
    "1) <img src=\"hidden\\photo1.png\" alt=\"This image is a representation of the simple neural network\" style=\"width: 600px;\"/>  <br />\n",
    "    1) To train a deep neural network, we need to send in a lot of examples in order for the neral network to generalize the pattern and learn from it. We can't just show it one instance of a certain situation (experience). We need to show it the same or similar instances over and over again before it can learn to overcome this challenge. We can use something called experience replay. \n",
    "    2) In this tutorial, the experience consists of five components:\n",
    "        1) state:\n",
    "            1) This component stores 12 state information of the current state, in the form of value.\n",
    "        2) action:\n",
    "            1) This component stores the action the agent (flappy bird) took, in the form of value (0 or 1).\n",
    "        3) new_sate:\n",
    "            1) This component stores 12 state information of the next state, in the form of value.\n",
    "        4) reward:\n",
    "            1) This component stores the amount of reward the agent received after taking that action to move into the next state from the current state.\n",
    "        5) terminated:    \n",
    "            1) This component stores the information whether it's game over or not (if the agent is dead after taking that action to move into the next state from the current state), in the form of value (0 or 1). \n",
    "    3) We take this combination as the experience and save it into this memory here, which is basically a python deque. A deck is a double-ended list. We keep adding experiences to the front of the deque. New/latest experience is kept being pushed into the deque and eventually the deque is going to get full, and the deque is going to start purging the old stuff (experience). \n",
    "    4) In other words, the deque practices first-in-first-out (FIFO). This way, even if we train the DQN for extremely long time, we'll never run out of memory. \n",
    "    5) When we want to train the neural network (DQN), we'll just grab a batch of experience samples from the deque (means only take a portion of experience out of all experience), at a certain batch size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define memory for Experience Replay\n",
    "from collections import deque\n",
    "import random\n",
    "\n",
    "## To sum up:\n",
    "# 1) deque is a memory structure that has a finite size. When a deque is full and you still store (append) new data into it, the last data stored in the deque will be purged out (removede) while the new data will be added to the first element of deque, and vice versa (In other words, deque practices first in first out [FIFO]).\n",
    "# 2) Here, we use deque to store the experience to train the DQN. The experience (similar to a set of features for neural network training) here is called transition. Transition is a variable stores the information of (state, action, new_state, reward, terminated).\n",
    "# 3) state -> the 12 state information of current state\n",
    "# 4) action -> the action (represented by either 0 or 1) that took by the agent to move into the next state from the current state\n",
    "# 5) new_state -> the 12 state information of next state\n",
    "# 6) reward -> the amount of reward the agent received after taking that action to move into the next state from the current state\n",
    "# 7) terminated -> if the game ended (or is the agent dead in the environment) after the agent taking that action to move into the next state from the current state\n",
    "\n",
    "\n",
    "class ReplayMemory():\n",
    "\n",
    "    # maxlen refers to the maximum length used to initialize the deque. seed is used to control the randomness (we can send in a seed to initialize random)\n",
    "    def __init__(self, maxlen, seed = None):\n",
    "        self.memory = deque([], maxlen = maxlen)\n",
    "\n",
    "        # Optional seed for reproducibility \n",
    "        if seed is not None:\n",
    "            random.seed(seed)\n",
    "\n",
    "    def append(self, transition):\n",
    "        self.memory.append(transition) # append() is used to append the experience to the memory (deque). The experience (features) here is called transition, it is a tuple of information consisting (state, action, new_state, reward, terminated), is used to train the DQN. \n",
    "\n",
    "    def sample(self, sample_size): # sample() will randomly select the samples in the transition (experience) then return them. The number of samples that randomly selected depends on the batch size we specified as sample_size.\n",
    "        return random.sample(self.memory, sample_size) \n",
    "    \n",
    "    def __len__(self): # __len__() will just return the length of the memory (deque)\n",
    "        return len(self.memory)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
