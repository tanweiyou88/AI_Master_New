{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implement Target Network (DQN PyTorch Beginners Tutorial 5, by Johny Code)\n",
    "\n",
    "Link to the Youtube video tutorial: https://www.youtube.com/watch?v=vYRpJo-KMSw&list=PL58zEckBH8fCMIVzQCRSZVPUp3ZAVagWi&index=5  <br />\n",
    "Link to the Gymnasium official website: https://gymnasium.farama.org/ <br />\n",
    "Link to the Flappy Bird environment GitHub repository: https://github.com/markub3327/flappy-bird-gymnasium <br />\n",
    "\n",
    "**Information of a Target network:**\n",
    "1) Reference: https://community.deeplearning.ai/t/target-network-clarification/326384/3\n",
    "2) <img src=\"hidden\\photo3.png\" alt=\"This image is a representation of the simple neural network\" style=\"width: 700px;\"/>  <br />\n",
    "\n",
    "**Deep Q-network (DQN) vs Target network:**\n",
    "1) Important notes: The output of the deep Q-network (DQN) is the predicted Q values for all actions (the predicted reward of taking the corresponding actions) and taget network are the target Q values for all actions (the ground truth reward of taking the corresponding actions) in the action space of the environment.\n",
    "2) The purpose of syncing both DQN and target network at every certain count step:\n",
    "    1) At the 1st sync, since we use mini batch here, the DQN is used to provide the predicted Q values for all actions at all instances in the memory (deque). The output of DQN is used get the loss function by comparing it with the outputs of the target network to train the target network. The target network is trained with the loss function & both the DQN and target network have the same weights and biases. At each iteration of different episodes before the next sync, only DQN training is conducted. The DQN will provide outputs with some randomness [because we implement Random Greedy algorithm] (predicted Q-values for taking different actions) based on the features in the memory (deque) it has, and also to collect more experience. But when more and more iteration performed, the data in the memory changed and this caused the DQN to provide different outputs (the predictions are fluactuated, so the DQN training can't be conducted stably in dynamic environment).\n",
    "    2) Hence, we need the trained target network to provide outputs (target Q values for all actions in the action space). Since the target network is synced (its weights and biases) and trained only once for every few step counts, the weigths and biases of the target network is remained constant for a period so that the target network can provide stable target values at that period for DQN training.\n",
    "    3) In other words, the target network outputs (target Q values) are used as the guidance (we now can calculate the loss function using the target Q values from the target network and predicted Q values from the DQN) to realize the backpropagation of DQN for its learning.\n",
    "    4) When approaching to the next sync, due to the backpropagation, the DQN is trained to provide better outputs (the outputs Q values are closer to the one of the target network). This also means the target network guides the DQN in its training.\n",
    "    5) After the 2nd sync, both the DQN and target network are trained and having the same weights and biases again. This time, the target network can provide better outputs (based on the new experience) to guide the DQN learning.\n",
    "    6) <img src=\"hidden\\photo0.png\" alt=\"This image is a representation of the simple neural network\" style=\"width: 300px;\"/>  <br />\n",
    "    7) <img src=\"hidden\\photo1.png\" alt=\"This image is a representation of the simple neural network\" style=\"width: 300px;\"/>  <br />\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Information of DQN Target Formula:**\n",
    "1) <img src=\"hidden\\photo2.png\" alt=\"This image is a representation of the simple neural network\" style=\"width: 600px;\"/>  <br />\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# This if-else block implements the DQN Target Formula:\n",
    "for state, action, new_state, reward, terminated in mini_batch: # get the data stored in the variable mini_batch\n",
    "        \n",
    "            # This if-else block implements the DQN Target Formula:\n",
    "            if terminated: # if the game is over\n",
    "                target = reward # the target (ground truth) reward = reward\n",
    "\n",
    "            else:  # if the game is not over\n",
    "                with torch.no_grad:\n",
    "                    target_q = reward + self.discount_factor_g * target_dqn(new_state).max() # the target (ground truth) reward"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
