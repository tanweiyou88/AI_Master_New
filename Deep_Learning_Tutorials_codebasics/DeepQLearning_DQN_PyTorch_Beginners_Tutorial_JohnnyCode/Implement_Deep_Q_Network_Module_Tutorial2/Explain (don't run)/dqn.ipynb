{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Information of Deep Q-Network (DQN)**\n",
    "1) <img src=\"hidden\\photo1.png\" alt=\"This image is a representation of the simple neural network\" style=\"width: 400px;\"/>  <br />\n",
    "    1) A deep Q-network is nothing more than a regular neural network with fully connected layers. \n",
    "    2) What makes it special is that the input layer represents the state (features) of the current state, so the state of the current state is the combination of the 12 pieces of state information in this Flappy Bird environment. So we would expect 12 nodes (input neurons). At the input layer, different input neurons receive different state information (1 input neuron receives 1 state information) in the form of value (normalized value) as the features, ranging from representing the last pipe (or the pipe that we just passed) to the player's rotation.\n",
    "    3) The output layer represents the actions. So in Flappy Bird environment, there are only 2 actions. We would expect 2 output neurons in this output layer. 1 output neuron represents the bird doing nothing while the other output neuron represents the bird flapping its wings. The value provided by each output neuron at the output layer is the Q value or Q for quality. The Q values are also called expected return or expected reward. For example, given a certain state, the output provided by each output neuron (which is the Q value or the expected reward the agent will receive if the agent takes that action) is something like 0.1 and 0.6 respectively. The highest Q value represents the best action the agent should take to move into next state, based on the given current state features. In this example, the best action is the agent to flap its wings and the agent is expected to get this amount of reward back after taking that best action to move into next state. \n",
    "    4) The goal of the algorithm in agent.py is to train this neural network (policy network) such that with any given current state features, the neural network ables to predict what actions are best for the agent to take in that state to move into next state. After training the policy network, given the state (features) of the current state, the policy dictates the best actions that the agent should take now to move into next state. \n",
    "    5) The number of hidden layers between the input and output layers and the number of neurons (nodes) in those hidden layers can vary depending on how complex your environment is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# **The standard way to create a neural network in pytorch is to define a class.**\n",
    "\n",
    "# Define a class for the neural network, called DQN. The class DQN inherit the nn.\n",
    "class DQN(nn.Module):\n",
    "\n",
    "    # The __init__ function defines the layers of a neural network\n",
    "    def __init__(self, state_dim, action_dim, hidden_dim = 256): # pass the arguments of input dimension (state_dim), output dimension (action_dim), and the number of neurons in the hidden layer (here, we set the number of neurons in the hidden layer as 256)\n",
    "        super(DQN, self).__init__()\n",
    "\n",
    "        # In pytorch, the input layer of a neural network (the 1st layer of a neural network) is implicit (means we don't need to write code for the input layer)\n",
    "\n",
    "        # Define the 1st hidden layer of the neural network (the second layer of the neural network). This layer is called fc1 in this script. [this layer is the Hidden Layer 1 shown in the video]\n",
    "        self.fc1 = nn.Linear(state_dim, hidden_dim)\n",
    "\n",
    "        # Define the output layer of the neural network (the third layer of the neural network). This layer is called fc2 in this script.\n",
    "        self.fc2 = nn.Linear(hidden_dim, action_dim)\n",
    "\n",
    "    # The forward function does the calculations within the neural network\n",
    "    def forward(self, x): # x is the input that contains the 12 state values\n",
    "        x = F.relu(self.fc1(x)) # we send the x (12 state values) through the fc1 layer, followed by passing through the relu activation function. The output of the relu activation function of this fc1 layer is stored in x\n",
    "        return self.fc2(x) # The output of the relu activation function of fc1 layer which is stored in x is provided to the output layer to calculate the Q values. The calculated Q values will be returned by this forward function.\n",
    "\n",
    "# Define the main function (analogous to the body of a script). The functions defined outside this main function play the supporting role when they are called inside this main function.    \n",
    "if __name__ == '__main__':\n",
    "    state_dim = 12 # the state dimension (input dimension) has size of 12, because there are 12 state information considered in observation (according to the Flappy Bird GitHub repository). In other words, when the agent (flappy bird) took an action and reached a new state, the agent will get 12 state values at that new state, where each state value represents different information/meaning observed at that new state.\n",
    "    action_dim = 2 # the action dimension (output dimension) has size of 2, because there are only 2 actions in the action space of the environemnt (according to the Flappy Bird GitHub repository)\n",
    "    net = DQN(state_dim, action_dim) # declare the neural network\n",
    "    state = torch.randn(1, state_dim) # create random input (total 12 random values will be provided, while each value represents a state value at the next(new) state after executing an action). If the flappy bird took 10 actions to move into 10 new states, we can simulate the 12 state values at each new state by having the code of \"torch.randn(10, state_dim)\". Then, we can pass the 12 state values at each new state = 12 * 10 = 120 state values in a batch to the neural network to calculate the corresponding Q values.\n",
    "    output = net(state) # send the random input into the neural network to calculate the Q values. The calculated Q values provided at the output layer of the neural network is returned and stored in variable output. Only 2 Q values will be returned by the neural network because the output dimension of the neural network is defined as size of 2 (means there are only 2 output neurons in the output layer of the neural network, with each output neuron provide a Q value. A Q value represents the probability of taking the corresponding action)\n",
    "    print(output) # print the calculated Q values stored in variable output."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
