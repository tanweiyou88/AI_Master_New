{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1 of \"Satellite image to maps translation using pix2pix (a variant of conditional GAN model)\": Create a pix2pix model\n",
    "\n",
    "1) Link to the Youtube video tutorial: https://www.youtube.com/watch?v=6pUSZgPJ3Yg&list=PLZsOBAyNTZwboR4_xj-n3K6XBTweC4YVD&index=7\n",
    "2) Link to the stallite-map images dataset: http://efrosgans.eecs.berkeley.edu/pix2pix/datasets/maps.tar.gz\n",
    "3) Link to other datasets that suitable for pix2pix training: http://efrosgans.eecs.berkeley.edu/pix2pix/datasets/\n",
    "\n",
    "4) Instruction: \n",
    "    1) This is the supplementary script of this tutorial. This script not only defines the generator, discriminator, and GAN models, but also all the functions required for the GAN model training. \n",
    "    2) Use the virtual environment called GPUEnv to run this script. This virtual environemnt uses Python 3.7.16 version, Tensorflow 2.6.0 version, Keras 2.6.0 version.\n",
    "    3) In this tutorial, a pix2pix is trained to perform image to image translation, by taking a real image (satellite image) as the source image and generate its corresponding fake target image (fake map image), while its real target image (involved in the model training) is its corresponding real map image.\n",
    "5) Definition:\n",
    "    1) Input image (also called Source image) = The image we want to convert to another image\n",
    "    2) Target image (also called real target image) = The image used to specify the image type we want the generator to generate after training\n",
    "    3) Generated [fake] image (also called fake target image) = The image generated by the generator network\n",
    "    4) A real sample = an input image concatenated with its corresponding target image\n",
    "    5) A fake sample = an input image concatenated with its corresponding fake image\n",
    "    6) The generator network takes an input image to generate a fake image\n",
    "    7) The discriminator network takes either a real sample or fake sample as the input and provide it a real sample probability score as the output\n",
    "\n",
    "6) **Explanation of pix2pix concept:**\n",
    "    1) pix2pix is a variant of Conditional GAN model that is trained to perform image to image translation (take a source image as the input and generate its corresponding fake target image as the output), while its real target image is involved in the model training.\n",
    "    2) <img src=\"hidden\\photo1.png\" alt=\"This image is a representation of the simple neural network\" style=\"width: 400px;\"/>  <br />\n",
    "    3) <img src=\"hidden\\photo2.png\" alt=\"This image is a representation of the simple neural network\" style=\"width: 300px;\"/>  <br />\n",
    "    4) <img src=\"hidden\\photo3.png\" alt=\"This image is a representation of the simple neural network\" style=\"width: 400px;\"/>  <br />\n",
    "    5) <img src=\"hidden\\photo4.png\" alt=\"This image is a representation of the simple neural network\" style=\"width: 400px;\"/>  <br />\n",
    "    6) According to the paper described in that Youtube tutorial video (https://www.youtube.com/watch?v=UcHe0xiuvpg&list=PLZsOBAyNTZwboR4_xj-n3K6XBTweC4YVD&index=6), the goal is using a pix2pix to generate a realistic looking image as the output, with a segmented version of the same image as the input. So:\n",
    "        1) The input image of the pix2pix [also called source image]: The segmented version of the same image\n",
    "            1) In this tutorial, the source image is the satellite image from the dataset.\n",
    "        2) The target image (TI) of the pix2pix [also called real target image/ground truth, in terms of if the appearance of each generated fake image following the type (style) we want]: The real-world version of the same image (the image is taken in real world scenario)\n",
    "            1) In this tutorial, the real target image is the corresponding map image from the dataset.\n",
    "        3) The fake image (FI) [also called fake target image, the image generated by the generator network] of the Pix2Pix GAN: The realistic looking image\n",
    "            1) In this tutorial, the fake target image is the corresponding map image generated by the generator network.\n",
    "    7) When training the discriminator network, each input image will be concatenated to each target image and fake image respectively as additional channels (to preserve the information/details of the input image [similar to the concept of skip connection]). Then, the discriminator network will be trained on a half batch size of real samples (target images concatenated with input images) with ground truth labels of 1 as real image  and a half batch size of fake samples (fake images concatenated with input images) with ground truth labels of 0 as fake image. Then, sigmoid cross entropy is the cost function used to update the weights and biases of the discriminator network.\n",
    "    8) When training the generator network, each input image will be concatenated to each fake image as additional channels (to preserve the information/details of the input image [similar to the concept of skip connection]). Then, the generator network will be trained at 2 parts:\n",
    "        1) Part 1 (involve generator network only): Calculate the means square error (MAE OR L1) between each target image and fake image (without concatenating with the input image). This MAE cost function value is like verifying how similar each generated fake image is to its corresponding target image, so later the generator network can improve its ability to generate each fake image that looks similar to its corresponding target image (ground truth, in terms of if the appearance of each generated fake image following the type (style) we want). \n",
    "        2) Part 2 (involve GAN model): Train the GAN model with a batch of fake samples (fake images concatenated with input images) with ground truth labels of 1 (to fool the discriminator network), then use the sigmoid cross entropy as the cost function. This sigmoid cross entropy as the cost function is like verifying how similar each fake sample (fake image concatenated with input image) is to its corresponding real sample (real image concatenated with input image).\n",
    "        3) Then the cost function value at both Part 1 and Part 2 will be used to update the weights and biases of the generator network. Part 1 is guiding the generator network to generate each fake image following the type (style) we want. Part 2 is guiding the generator network to generate each fake image such that after each fake image is concatenated with its input image to become a fake sample, the fake sample preserves the information (details) of the input image and has the appearance following the type (style) we want.\n",
    "\n",
    "7) **Explanation of receptive field concept:**\n",
    "    1) <img src=\"hidden\\photo5.png\" alt=\"This image is a representation of the simple neural network\" style=\"width: 400px;\"/>  <br />\n",
    "    2) Here, the last layer is connected after the C512 layer (means the C512 layer is before the last layer). By using the output size, stride and kernel size of the last layer, we calculate the receptive field of the last layer using the receptive field formula, and we get the receptive field at the last layer is 4. This means standing at a location (pixel) of the last layer, we observe a region of size 4x4 pixels of the C512 layer. In other words, each pixel on the last layer is the operation result (containing the relationship of) involving a patch of 4x4 pixels of the C512 layer. \n",
    "    3) The C64 layer is connected after the input layer of the discriminator network (consists each sample as an 256x256 image). Similarly, by using the output size, stride and kernel size of the C64 layer, we calculate the receptive field of the C64 layer using the receptive field formula, and we get the receptive field at the C64 layer is 70. This means each pixel on the C64 layer is the operation result (containing the relationship of) involving a patch of 70x70 pixels of the input layer (each 256x256 input image). That's is the reason this discriminator network is part of the PatchGAN model.\n",
    "    4) <img src=\"hidden\\photo6.png\" alt=\"This image is a representation of the simple neural network\" style=\"width: 400px;\"/>  <br />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import zeros\n",
    "from numpy import ones\n",
    "from numpy.random import randint\n",
    "from keras.optimizers import adam_v2\n",
    "from keras.initializers import RandomNormal\n",
    "from keras.models import Model\n",
    "from keras.models import Input\n",
    "from keras.layers import Conv2D\n",
    "from keras.layers import Conv2DTranspose\n",
    "from keras.layers import LeakyReLU\n",
    "from keras.layers import Activation\n",
    "from keras.layers import Concatenate\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import BatchNormalization\n",
    "from matplotlib import pyplot as plt\n",
    "from tensorflow.keras.utils import plot_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the function to create/build/define and compile the discriminator network\n",
    "\n",
    "1) Here, the discriminator network is a PatchGAN\n",
    "2) We use functional way of defining model as we have multiple inputs; both images and corresponding labels.\n",
    "3) When we perform convolution operation on an image, we are reducing the image dimension (dowmsampling).\n",
    "4) Given an input image (feature of image) and an input label (class of image), the Discriminator(discriminator network) outputs the likelihood(probability) of the image being real.\n",
    "5) Unlike the unconditional (regular) GAN model, here we are also providing number of classes (CIFAR10 class) as input to the discriminator network. Hence, Input to the discriminator model will be both images and labels. \n",
    "6) in_label = Input(shape=(1,)) means we initialize the variable in_label as the input layer of the model. This input layer accepts each input sample that has size of 1 (a single value), but the batch size of the input layer is omitted (means you can supply as many samples as you want as the input, but each sample must have shape of 1). Hence, basically, when we define layers (structure) of a model, we only consider the size/dimension of a sample without considering the batch size (when the batch size argument is omitted) [means we think of if a sample of dimension x is provided to a layer as input, then the corresponding output becomes what dimension, then we assume a batch of samples at that stage have the same dimension as we described earlier]\n",
    "7) model = Model([in_image, in_label], out_layer) # Here we just define like the model created using Model() has 2 input layers (in_image layer that accepts each input sample of dimension in_shape & in_label layer that accepts each input sample of dimension (1,1)) and 1 output layer (out_layer layer that provide each output sample of dimension (1,1)). We only consider a sample and omit the batch size here (because the batch size argument is omitted when we define both input layers). However, when deploying the model, we can provide a batch size of samples to both of the input layers, and the same batch size of samples will be generated at the output layer, while the dimension of each sample in the batch at different layers follows the ones we set during developing the layers of the model. When we want to deploy the model to make prediction, we just replace the name of the input layer with the variable names that store the training data (features). When we want to deploy the model for training, we just replace the name of the input layer with the variable names that store the training data (features) + the variable name that stores the ground truths.\n",
    "    1) Reference: https://keras.io/guides/functional_api/\n",
    "\n",
    "8) **Important information when defining the layers (structure) of a model:**\n",
    "    1) When defining the layers (structure) of a model, we only consider the shape of each sample provided at the input layer, before and after each layer, and generated at the output layer but omit the batch size (if the batch size argument is omitted when we define the input layers). Means all about the shape of each sample at a particular layer. \n",
    "    2) However, when deploying the model, we can provide a batch size of samples to the input layer, and the same batch size of samples will be generated at the output layer. While the dimension of each sample in the batch at different layers follows the ones we set during developing the layers of the model.\n",
    "    3) <img src=\"hidden\\photo1.png\" alt=\"This image is a representation of the simple neural network\" style=\"width: 400px;\"/>  <br />\n",
    "        1) Reference: https://keras.io/guides/functional_api/\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since pix2pix is a conditional GAN, it takes 2 inputs - image and corresponding label (target image)\n",
    "# For pix2pix, the label will be another image, called target image. \n",
    "\n",
    "# Define the standalone discriminator model\n",
    "# Given an image sample (each target/generated(fake) image concatenated with its corresponding input image) at the Discriminator's input layer, the Discriminator outputs the likelihood of the image sample being real.\n",
    "# Binary classification - true or false (1 or 0). So using sigmoid activation.\n",
    "# Think of discriminator as a binary classifier that is classifying image samples at its input layer as real/fake.\n",
    "\n",
    "# From the paper C64-C128-C256-C512 (C[value] refers to convolution layer, value refers to the number of filters available in that convolution layer)\n",
    "# At the last (output) layer, perform convolution to downsample the input vector of that layer into a 1-dimensional output, followed by a Sigmoid function.  \n",
    "\n",
    "def define_discriminator(image_shape):\n",
    "    \n",
    "\t# Weight initialization\n",
    "\tinit = RandomNormal(stddev=0.02) # As described in the original paper\n",
    "    \n",
    "\t# Input 1: Input image (also called Source image) (The image we want to convert to another image)\n",
    "\tin_src_image = Input(shape=image_shape) # This line initializes the variable in_src_image as the input layer of the model. Each sample provided to this input layer must have size of image_shape, but the batch size of this input layer is omitted (means you can supply any batch size of samples as you want to this input layer as the input, but each sample must have shape of image_shape).\n",
    "\t# Input 2: Target image (The image used to specify the image type we want the generator to generate after training)\n",
    "\tin_target_image = Input(shape=image_shape)  # This line initializes the variable in_target_image as the input layer of the model. Each sample provided to this input layer must have size of image_shape, but the batch size of this input layer is omitted (means you can supply any batch size of samples as you want to this input layer as the input, but each sample must have shape of image_shape).\n",
    "    \n",
    "\t# Concatenate (channel-wise) each target image with its corresponding input image as a real sample (which will be supplied to the input layer of the discriminator) \n",
    "\tmerged = Concatenate()([in_src_image, in_target_image])\n",
    "    \n",
    "\t# Here, the discriminator network receives each real sample (input image + target image) to classify if each of the received sample is a real sample.\n",
    "\t# C64: 4x4 kernel Stride 2x2\n",
    "\td = Conv2D(64, (4,4), strides=(2,2), padding='same', kernel_initializer=init)(merged) # Means each input of this convolution layer (merged) is a real sample of  has shape of (32,32,4), each output sample of this convolution layer is (d)\n",
    "\td = LeakyReLU(alpha=0.2)(d)\n",
    "\t# C128: 4x4 kernel Stride 2x2\n",
    "\td = Conv2D(128, (4,4), strides=(2,2), padding='same', kernel_initializer=init)(d)\n",
    "\td = BatchNormalization()(d)\n",
    "\td = LeakyReLU(alpha=0.2)(d)\n",
    "\t# C256: 4x4 kernel Stride 2x2\n",
    "\td = Conv2D(256, (4,4), strides=(2,2), padding='same', kernel_initializer=init)(d)\n",
    "\td = BatchNormalization()(d)\n",
    "\td = LeakyReLU(alpha=0.2)(d)\n",
    "\t# C512: 4x4 kernel Stride 2x2 \n",
    "    # Not in the original paper. Comment this block if you want.\n",
    "\td = Conv2D(512, (4,4), strides=(2,2), padding='same', kernel_initializer=init)(d)\n",
    "\td = BatchNormalization()(d)\n",
    "\td = LeakyReLU(alpha=0.2)(d)\n",
    "\t# second last output layer : 4x4 kernel but Stride 1x1\n",
    "\td = Conv2D(512, (4,4), padding='same', kernel_initializer=init)(d)\n",
    "\td = BatchNormalization()(d)\n",
    "\td = LeakyReLU(alpha=0.2)(d)\n",
    "\t# The output layer provides image patches (means the discriminator network will split an image into image patches, then identify each image patch if it is a real image patch)\n",
    "\td = Conv2D(1, (4,4), padding='same', kernel_initializer=init)(d)\n",
    "\t# The real sample patch probability score for each input image patch\n",
    "\tpatch_out = Activation('sigmoid')(d)\n",
    "\t# Define model\n",
    "\tmodel = Model([in_src_image, in_target_image], patch_out) # Here we just define like the model created using Model() has 2 input layers (in_src_image layer & in_target_image layer) and 1 output layer (patch_out layer). We omit the batch size here (because the batch size argument is omitted when we define both input layers). However, we can provide a batch size of samples to both of the input layers, and the same batch size of samples will be generated at the output layer, while the dimension of each sample in the batch at different layers follows the ones we set during developing the layers of the model.\n",
    "\t# Compile model\n",
    "    # The model is trained with a batch size of one image and Adam opt, with a small learning rate and 0.5 beta (The loss for the discriminator is weighted by 50% for each model update). \n",
    "    \n",
    "\topt = adam_v2.Adam(lr=0.0002, beta_1=0.5)\n",
    "\tmodel.compile(loss='binary_crossentropy', optimizer=opt, loss_weights=[0.5]) \n",
    "\treturn model\n",
    "\n",
    "# disc_model = define_discriminator((256,256,3))\n",
    "# plot_model(disc_model, to_file='disc_model.png', show_shapes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the function to create/build/define the generator network\n",
    "\n",
    "1) Here, the generator network is a U-Net.\n",
    "2) **Important information when defining the layers (structure) of a model:**\n",
    "    1) We only consider the shape of each sample provided at the input layer, before and after each layer, and generated at the output layer but omit the batch size (if the batch size argument is omitted when we define the input layers). Means all about the shape of each sample at a particular layer. \n",
    "    2) However, when deploying the model, we can provide a batch size of samples to the input layer, and the same batch size of samples will be generated at the output layer. While the dimension of each sample in the batch at different layers follows the ones we set during developing the layers of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now define the generator - in our case we will define a U-net (consists of an encoder and a decoder) as the generator\n",
    "# Define an encoder block to be used in generator\n",
    "def define_encoder_block(layer_in, n_filters, batchnorm=True):\n",
    "\t# Weight initialization\n",
    "\tinit = RandomNormal(stddev=0.02)\n",
    "\t# Add downsampling layer (convolution layer)\n",
    "\tg = Conv2D(n_filters, (4,4), strides=(2,2), padding='same', kernel_initializer=init)(layer_in)\n",
    "\t# Conditionally add batch normalization\n",
    "\tif batchnorm:\n",
    "\t\tg = BatchNormalization()(g, training=True)\n",
    "\t# Leaky relu activation function\n",
    "\tg = LeakyReLU(alpha=0.2)(g)\n",
    "\treturn g\n",
    "\n",
    "# Define a decoder block to be used in generator\n",
    "def decoder_block(layer_in, skip_in, n_filters, dropout=True):\n",
    "\t# Weight initialization\n",
    "\tinit = RandomNormal(stddev=0.02)\n",
    "\t# Add upsampling layer (deconvolution layer)\n",
    "\tg = Conv2DTranspose(n_filters, (4,4), strides=(2,2), padding='same', kernel_initializer=init)(layer_in)\n",
    "\t# Add batch normalization\n",
    "\tg = BatchNormalization()(g, training=True)\n",
    "\t# Conditionally add dropout\n",
    "\tif dropout:\n",
    "\t\tg = Dropout(0.5)(g, training=True)\n",
    "\t# Merge with skip connection\n",
    "\tg = Concatenate()([g, skip_in]) # This skip connection makes the generator as a unit (like a module), not just a regular encoder decoder.\n",
    "\t# Relu activation function\n",
    "\tg = Activation('relu')(g)\n",
    "\treturn g\n",
    "\n",
    "# Define the standalone generator model - U-net\n",
    "def define_generator(image_shape=(256,256,3)):\n",
    "\t# Weight initialization\n",
    "\tinit = RandomNormal(stddev=0.02)\n",
    "\t# Input image\n",
    "\tin_image = Input(shape=image_shape) # This line initializes the variable in_image as the input layer of the model. Each sample provided to this input layer must have size of image_shape, but the batch size of this input layer is omitted (means you can supply any batch size of samples as you want to this input layer as the input, but each sample must have shape of image_shape).\n",
    "\t# Encoder model: C64(First layer)-C128-C256-C512-C512-C512-C512-C512(Last layer)\n",
    "\te1 = define_encoder_block(in_image, 64, batchnorm=False)\n",
    "\te2 = define_encoder_block(e1, 128)\n",
    "\te3 = define_encoder_block(e2, 256)\n",
    "\te4 = define_encoder_block(e3, 512)\n",
    "\te5 = define_encoder_block(e4, 512)\n",
    "\te6 = define_encoder_block(e5, 512)\n",
    "\te7 = define_encoder_block(e6, 512)\n",
    "\t# Bottleneck (The base of the U-Net. This variable stores the reduced dimension of the given input image, similar to the one of autoencoder), no batch norm and relu\n",
    "\tb = Conv2D(512, (4,4), strides=(2,2), padding='same', kernel_initializer=init)(e7) # Means each input of this convolution layer (e7) is a feature map of the input image, each output sample of this convolution layer (d) is a smaller feature map of the same input image\n",
    "\tb = Activation('relu')(b)\n",
    "\t# Decoder model: CD512(First layer)-CD512-CD512-C512-C256-C128-C64(Last layer)\n",
    "\td1 = decoder_block(b, e7, 512)\n",
    "\td2 = decoder_block(d1, e6, 512)\n",
    "\td3 = decoder_block(d2, e5, 512)\n",
    "\td4 = decoder_block(d3, e4, 512, dropout=False)\n",
    "\td5 = decoder_block(d4, e3, 256, dropout=False)\n",
    "\td6 = decoder_block(d5, e2, 128, dropout=False)\n",
    "\td7 = decoder_block(d6, e1, 64, dropout=False)\n",
    "\t# Output layer\n",
    "\tg = Conv2DTranspose(image_shape[2], (4,4), strides=(2,2), padding='same', kernel_initializer=init)(d7) #Modified \n",
    "\tout_image = Activation('tanh')(g)  # Generates each image whose pixel values are in the range from -1 to 1 (because of Tanh activation function). So also change (preprocess/scale) the pixel values of each input image and target image into the range from -1 to 1.\n",
    "\t# define model\n",
    "\tmodel = Model(in_image, out_image) # Here we just define like the model created using Model() has 1 input layer (in_image layer) and 1 output layer (out_image layer). We omit the batch size here (because the batch size argument is omitted when we define both input layers). However, we can provide a batch size of samples to both of the input layers, and the same batch size of samples will be generated at the output layer, while the dimension of each sample in the batch at different layers follows the ones we set during developing the layers of the model.\n",
    "\treturn model\n",
    "\n",
    "# gen_model = define_generator((256,256,3))\n",
    "# plot_model(gen_model, to_file='gen_model.png', show_shapes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the function to create/build/define and compile the GAN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generator network is trained via GAN combined model. \n",
    "# Define the GAN model (combined generator and discriminator model), for updating the weights and biases of the generator network\n",
    "# Discriminator network is trained separately so here only generator network will be trained by keeping the weights and biases of the discriminator network constant. \n",
    "# \n",
    "# Important information when defining the layers (structure) of a model:\n",
    "# 1) We only consider the shape of a sample provided at the input layer, before and after a layer, and generated at the output layer but omit the batch size (if the batch size argument is omitted when we define the input layers). \n",
    "# 2) However, when deploying the model, we can provide a batch size of samples to the input layer, and the same batch size of samples will be generated at the output layer. While the dimension of each sample in the batch at different layers follows the ones we set during developing the layers of the model.\n",
    "#\n",
    "def define_gan(g_model, d_model, image_shape):\n",
    "\t# Make weights in the discriminator not trainable\n",
    "\tfor layer in d_model.layers:\n",
    "\t\tif not isinstance(layer, BatchNormalization):\n",
    "\t\t\tlayer.trainable = False       # Descriminator layers set to untrainable in the GAN model but standalone descriminator will be trainable.\n",
    "            \n",
    "\t# Input image (also called Source image) (The image we want to convert to another image)\n",
    "\tin_src = Input(shape=image_shape)  # This line initializes the variable in_src as the input layer of the model. Each sample provided to this input layer must have size of image_shape, but the batch size of this input layer is omitted (means you can supply any batch size of samples as you want to this input layer as the input, but each sample must have shape of image_shape).\n",
    "\t# Suppy each input (source) image as input to the generator \n",
    "\tgen_out = g_model(in_src) # Define the created generator network as a layer of the model to provide each fake sample (each generated(fake) image concatenated with its corresponding input image). Means by omitting the sample batch size (means you can provide any batch size of samples you want), the generator network will provide each fake sample of shape image_shape that is stored in the variable gen_out.\n",
    "\t# Supply each input image and generated(fake) image as inputs to the discriminator\n",
    "\tdis_out = d_model([in_src, gen_out]) # Define the created discriminator network as a layer of the model to provide real image sample probability score. Means by omitting the sample batch size (means you can provide any batch size of samples you want), the discriminator network will receive each input image sample at the input layer called in_src & receive each generated (fake) image sample at the input layer called gen_out. Then, the discriminator network generates each output sample of shape (1,1) [which is the real sample classification probability score].\n",
    "\t# src image as input, generated image and disc. output as outputs\n",
    "\tmodel = Model(in_src, [dis_out, gen_out]) # Here we just define like the model created using Model() has 1 input layer (in_src layer) and 2 output layers (dis_out layer & gen_out layer). We omit the batch size here (because the batch size argument is omitted when we define both input layers). However, when we want to deploy the model, we can provide any batch size of samples to both of the input layers, and the same batch size of samples will be generated at the output layer, while the dimension of each sample in the batch at different layers follows the ones we set during developing the layers of the model. When we want to deploy the model to make prediction, we just replace the name of the input layer with the variable names that store the training data (features). When we want to deploy the model for training, we just replace the name of the input layer with the variable names that store the training data (features) + the variable name that stores the ground truths.\n",
    "\t# Compile the GAN model\n",
    "\topt = adam_v2.Adam(lr=0.0002, beta_1=0.5)\n",
    "    \n",
    "    #Total loss is the weighted sum of adversarial loss (BCE) and L1 loss (MAE). Authors suggested weighting BCE vs L1 as 1:100.\n",
    "\tmodel.compile(loss=['binary_crossentropy', 'mae'], \n",
    "               optimizer=opt, loss_weights=[1,100])\n",
    "\t\n",
    "\treturn model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define a function to randomly select a batch of real image samples (input images and their corresponding target images), then assign each of their image patch a value of 1 as ground truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select a batch of random input and target images (samples), returns images and target\n",
    "def generate_real_samples(dataset, n_samples, patch_shape):\n",
    "\t# Unpack dataset (contains input images and their corresponding target images)\n",
    "\ttrainA, trainB = dataset # The variable trainA stores a batch of input images, the variable trainB stores the same batch size of the corresponding target image\n",
    "\t# Choose/select random instances\n",
    "\tix = randint(0, trainA.shape[0], n_samples) # Select n_samples numbers of input images, by randomly generating n_samples numbers of integers between the range from 0 (low, inclusive) to trainA.shape[0] (high, exclusive). Each randomly generated interger by the function represents the index of the input image features in the variable trainA and the index of the corresponding target image features in the variable trainB. The randomly generated indices are stored in the variable ix. Since each integer is randomly generated between the given range, it is possible to have multiple same integer (index of an input image/sample and index of its corresponding target image/sample) in ix, so that it is possible to use multiple same input and target image sets for training.\n",
    "\t# Load (retrieve) the selected input and target images\n",
    "\tX1, X2 = trainA[ix], trainB[ix] # The variable X1 stores the selected input image features, the variable X2 stores the selected target image features\n",
    "\t# Generate 'real sample' class labels (ground truths of 1). Label=1 indicating they are real\n",
    "\ty = ones((n_samples, patch_shape, patch_shape, 1)) # Here, for each image, all of its patches (with dimension of patch_shape x patch_shape pixels) are assiged with the ground truth value (so the variable y is a 3D array). This is because the discriminator we used here is a PatchGAN, which identifies if each image patch of a specific size is real (instead of identifying if the whole image itself [as a patch] is real in one-shot) \n",
    "\treturn [X1, X2], y # Returns a group of randomly selected input image features, their corresponding target image features, and their corresponding class labels (values of 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define a function to randomly generate a batch of fake images, then assign each of their image patch a value of 0 as ground truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a batch of fake images, returns images and targets\n",
    "def generate_fake_samples(g_model, samples, patch_shape):\n",
    "\t# Generate each fake instance(image) using each input image as the input\n",
    "\tX = g_model.predict(samples) # The variable samples contains the input image. Deploy the generator model to take each input image and generate each fake image. Each generated fake image is then stored in the variable X.\n",
    "\t# Generate 'fake sample' class labels (ground truth of 0)\n",
    "\ty = zeros((len(X), patch_shape, patch_shape, 1)) # Here, for each image, all of its patches (with dimension of patch_shape x patch_shape pixels) are assiged with the ground truth value (so the variable y is a 3D array). This is because the discriminator we used here is a PatchGAN, which identifies if each image patch of a specific size is real (instead of identifying if the whole image itself [as a patch] is real in one-shot) \n",
    "\treturn X, y # Returns a group of generated images and their corresponding class labels (values of 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define a function to periodically evaluate the generator model training process\n",
    "\n",
    "A GAN model is not like your regular training where it converges. It is like if the loss doesn't increase by so much, go ahead to stop GAN model training. This is because you're training your generator against discriminator, so you want to find a good balance between the generator and discriminator. So it's a bit challenging/tough to find out when this GAN model (entire thing) is converging. That's why you typically train your GAN model for a long time (a real long time) and then save the generator model periodically. Then, also generate plots (fake images) periodically to make sure the GAN model training (generator training) are heading in the right direction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function is just for the purpose of periodically come back and plot how the images generated by the generator network are looking while you're training the GAN model.\n",
    "# Generator generates samples and save the samples as a plot, and also save the generator model.\n",
    "# GAN models do not converge, we just want to find a good balance between the generator and the discriminator. Therefore, it makes sense to periodically save the generator model and check how good the generated image looks. \n",
    "def summarize_performance(step, g_model, dataset, n_samples=3): # g_model refers to the created generator network, dataset contains a batch of randomly selected input images and their corresponding target images\n",
    "\t# Load a batch/group (size of n_samples) of randomly selected input image features and their corresponding target image features from the variable dataset [ignore their corresponding class labels (values of 1)]\n",
    "\t[X_realA, X_realB], _ = generate_real_samples(dataset, n_samples, 1) # The variable X_realA stores the selected input image features, he variable X_realB stores the selected target image features. \"1\" refers to each dimension of the image patch.\n",
    "\t# Generate a batch of generated(fake) images [ignore their corresponding class labels (values of 0)]\n",
    "\tX_fakeB, _ = generate_fake_samples(g_model, X_realA, 1) # The variable X_fakeB stores the generated (fake) images. \"1\" refers to each dimension of the image patch.\n",
    "\t# Scale all pixel values from [-1,1] to [0,1] (so we can plot the images to visualize them)\n",
    "\tX_realA = (X_realA + 1) / 2.0\n",
    "\tX_realB = (X_realB + 1) / 2.0\n",
    "\tX_fakeB = (X_fakeB + 1) / 2.0\n",
    "\t# Plot real source images (also known as input images, the image we want to convert to another image)\n",
    "\tfor i in range(n_samples):\n",
    "\t\tplt.subplot(3, n_samples, 1 + i)\n",
    "\t\tplt.axis('off')\n",
    "\t\tplt.imshow(X_realA[i])\n",
    "\t# Plot generated target images (also known as fake images, generated by the generator network)\n",
    "\tfor i in range(n_samples):\n",
    "\t\tplt.subplot(3, n_samples, 1 + n_samples + i)\n",
    "\t\tplt.axis('off')\n",
    "\t\tplt.imshow(X_fakeB[i])\n",
    "\t# Plot real target image (also known as target images, the image used to specify the image type we want the generator to generate after training)\n",
    "\tfor i in range(n_samples):\n",
    "\t\tplt.subplot(3, n_samples, 1 + n_samples*2 + i)\n",
    "\t\tplt.axis('off')\n",
    "\t\tplt.imshow(X_realB[i])\n",
    "\t# Save plot to file\n",
    "\tfilename1 = 'plot_%06d.png' % (step+1)\n",
    "\tplt.savefig(filename1)\n",
    "\tplt.close()\n",
    "\t# Save the generator model\n",
    "\tfilename2 = 'model_%06d.h5' % (step+1)\n",
    "\tg_model.save(filename2)\n",
    "\tprint('>Saved: %s and %s' % (filename1, filename2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train pix2pix (conditional GAN) model\n",
    "\n",
    "1) In machine learning, an epoch is a complete pass through the dataset, while a batch is a subset of the dataset processed in one go. \n",
    "2) The batch size (refers to n_batch here) determines how many samples are processed in a batch, and the number of epochs (refers to n_epochs here) represents the total number of passes through the training dataset. \n",
    "3) The number of batches per training epoch = (total number of samples in the dataset) / (batch size)\n",
    "4) For the pix2pix model training:\n",
    "    1) For the discriminator training using real samples, the training is performed by taking the randomly selected input images [X_realA] and their corresponding target images [X_realB] as the features, while taking the 'real sample' class labels (ground truths of 1) [y_real] as the target (ground truth). The 'real sample' class labels (ground truths of 1) [y_real] is involved in calculating the adversarial loss (BCE).\n",
    "    2) For the discriminator training using fake samples, the training is performed by taking the randomly selected input images [X_realA] and the fake imges randomly generated by the generator network [X_fakeB] as the features, while taking the taking the 'fake sample' class labels (ground truths of 0) [y_fake] as the target (ground truth). The 'real sample' class labels (ground truths of 1) [y_real] is involved in calculating the adversarial loss (BCE).\n",
    "    3) The generator network training is performed through using the GAN model, by taking the randomly selected input images as the feature, while taking the 'real sample' class labels (ground truths of 1) and the fake imges randomly generated by the generator network as the targets (ground truths). The 'real sample' class labels (ground truths of 1) [y_real] is involved in calculating the adversarial loss (BCE);the fake imges randomly generated by the generator network [X_realB] is involved in calculating the L1 loss (MAE)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train pix2pix (conditional GAN) models\n",
    "def train(d_model, g_model, gan_model, dataset, n_epochs=100, n_batch=1): # d_model refers to the created discriminator network, g_model refers to the created generator network, gan_model refers to the created GAN model, dataset contains a batch of randomly selected input images and their corresponding target images, n_epochs refers to the total number of epochs for GAN model training (here, default[if no input for this argument] is 100), n_batch refers to the number of samples in a batch of samples (also means the batch size) (here, default[if no input for this argument] is 1, means only 1 sample out of all the training samples is prossed in one go). \n",
    "\t# Determine each image patch shape output by the discriminator network\n",
    "\tn_patch = d_model.output_shape[1]\n",
    "\t# Unpack dataset (contains input images and their corresponding target images)\n",
    "\ttrainA, trainB = dataset # The variable trainA stores a batch of input images, the variable trainB stores the same batch size of the corresponding target image\n",
    "\t# Calculate the number of batches per training epoch (means calculate need to split all the samples for training into how many batch). n_batch refers to the number of samples for training in each batch (can treat a batch like a small group).\n",
    "\tbat_per_epo = int(len(trainA) / n_batch)\n",
    "\t# Calculate the number of training iterations\n",
    "\tn_steps = bat_per_epo * n_epochs # number of steps = number of epochs\n",
    "\t# Manually enumerate epochs\n",
    "\tfor i in range(n_steps):\n",
    "\t\t# select a batch of real samples\n",
    "\t\t[X_realA, X_realB], y_real = generate_real_samples(dataset, n_batch, n_patch)\n",
    "\t\t# generate a batch of fake samples\n",
    "\t\tX_fakeB, y_fake = generate_fake_samples(g_model, X_realA, n_patch)\n",
    "\t\t# Update discriminator for real samples [a real sample = an input image + its corresponding target image] (here, we update the parameters [weights and biases] of the discriminator network everytime after it has processed a batch of samples [by default, only 1 sample in a batch])\n",
    "\t\td_loss1 = d_model.train_on_batch([X_realA, X_realB], y_real) # The discriminator training is performed by taking the randomly selected input images and their corresponding target images as the features, while taking the 'real sample' class labels (ground truths of 1) as the target (ground truth)\n",
    "\t\t# Update discriminator for generated (fake) samples [a fake sample = an input image + its corresponding fake image] (here, we update the parameters [weights and biases] of the discriminator network everytime after it has processed a batch of samples [by default, only 1 sample in a batch])\n",
    "\t\td_loss2 = d_model.train_on_batch([X_realA, X_fakeB], y_fake) # The discriminator training is performed by taking the randomly selected input images and the fake imges randomly generated by the generator network as the features, while taking the 'fake sample' class labels (ground truths of 0) as the target (ground truth)\n",
    "\t\t# Update the generator (here, we update the parameters [weights and biases] of the generator network everytime after it has processed a batch of samples [by default, only 1 sample in a batch])\n",
    "\t\tg_loss, _, _ = gan_model.train_on_batch(X_realA, [y_real, X_realB]) # The generator network training is performed through using the GAN model, by taking the randomly selected input images as the feature, while taking the 'real sample' class labels (ground truths of 1) and the fake imges randomly generated by the generator network as the targets (ground truths). the 'real sample' class labels (ground truths of 1) [y_real] is involved in calculating the adversarial loss (BCE);the fake imges randomly generated by the generator network [X_realB] is involved in calculating the L1 loss (MAE).\n",
    "\t\t# Summarize the GAN model performance by printing the losses at each iteration\n",
    "\t\tprint('>%d, d1[%.3f] d2[%.3f] g[%.3f]' % (i+1, d_loss1, d_loss2, g_loss))\n",
    "\t\t# Summarize the generator model performance by showing its generated images periodically\n",
    "\t\tif (i+1) % (bat_per_epo * 10) == 0:\n",
    "\t\t\tsummarize_performance(i, g_model, dataset)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GPUEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
