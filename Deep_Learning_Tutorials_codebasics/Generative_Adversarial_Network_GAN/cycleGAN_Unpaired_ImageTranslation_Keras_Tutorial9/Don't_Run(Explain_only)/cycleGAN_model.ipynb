{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1 of \"Unpaired image to image translation​ using cycleGAN in keras\": Create a cycleGAN model\n",
    "\n",
    "1) Link to the Youtube video tutorial: https://www.youtube.com/watch?v=2MSGnkir9ew&list=PLZsOBAyNTZwboR4_xj-n3K6XBTweC4YVD&index=10\n",
    "2) Link to the dataset (horse2zebrea): https://www.kaggle.com/datasets/balraj98/horse2zebra-dataset?resource=download\n",
    "\n",
    "3) Instruction: \n",
    "    1) This is the supplementary script of this tutorial. This script not only defines the generator, discriminator, and GAN models, but also all the functions required for the GAN model training. \n",
    "    2) Use the virtual environment called GPUEnv to run this script. This virtual environemnt uses Python 3.7.16.\n",
    "\n",
    "4) **Concept of cycle consistency:**\n",
    "    1) <img src=\"hidden\\photo1.png\" alt=\"This image is a representation of the simple neural network\" style=\"width: 400px;\"/>  <br />\n",
    "    2) Given that Generator 1 takes real domain 1 (horse) image to generate fake domain 2 (zebra) image; Generator 2 takes real domain 2 (zebra) image to generate fake domain 1 (horse) image.\n",
    "    3) When we provide a real domain 1 image to the Generator 1 to generate a fake domain 2 image, and that generated fake domain 2 image is provided to the Generator 2, the Generator 2 should generate an image that looks same as the real domain 1 image (which is provided to the Generator 1 previously) back.\n",
    "    4) <img src=\"hidden\\photo2.png\" alt=\"This image is a representation of the simple neural network\" style=\"width: 400px;\"/>  <br />\n",
    "        1) If you say path A (a real domain 1 image as input -> the Generator 2 generates a fake domain 2 image -> the Generator 1 generates an image that looks same as the real domain 1 image, which is provided to the Generator 2 previously ) is a cycle forward, then path B (a real domain 2 image as input -> the Generator 1 generates a fake domain 1 image -> the Generator 2 generates an image that looks same as the real domain 2image, which is provided to the Generator 1 previously) is a cycle backward, and vice versa.\n",
    "\n",
    "5) **Concept of identity mapping:**\n",
    "    1) <img src=\"hidden\\photo3.png\" alt=\"This image is a representation of the simple neural network\" style=\"width: 400px;\"/>  <br />  \n",
    "    2) Given that Generator 1 takes real domain 1 (horse) image to generate fake domain 2 (zebra) image; Generator 2 takes real domain 2 (zebra) image to generate fake domain 1 (horse) image.\n",
    "    3) When we provide an image that looks like domain 2 to the Generator 1, the Generator 1 should convert that image into an image that looks like domain 1. On the contrary, when we provide an image that looks like domain 1 to the Generator 1, the Generator 1  should give that exact same image back without performing any translation/operation. And vice versa for the case of the Generator 2.\n",
    "    The identity mapping loss also helps to preserve color composition between the input and output.\n",
    "    4) <img src=\"hidden\\photo4.png\" alt=\"This image is a representation of the simple neural network\" style=\"width: 400px;\"/>  <br />  \n",
    "\n",
    "6) **Information of cycleGAN architecture:**\n",
    "    1) A cycleGAN has 2 generators and 2 discriminators (2 generator-discriminator pairs). Each generator is a encoder-decoder unit that has consists of few ResNet blocks, and each discriminator is a PatchGAN.\n",
    "    2) Generator1-Discriminator1 pair:\n",
    "        1) Generator1 is trained to take each real domain 2 image to generate a fake domain 1 image\n",
    "        2) Discriminator1 is trained to take:\n",
    "            1) each real domain 1 image and verify if it is real\n",
    "            2) each fake domain 1 image (generated by the Generator1) and verify if it is real\n",
    "    3) Generator2-Discriminator2 pair:\n",
    "        1) Generator2 is trained to take each real domain 1 image to generate a fake domain 2 image\n",
    "        2) Discriminator2 is trained to take:\n",
    "            1) each real domain 2 image and verify if it is real\n",
    "            2) each fake domain 2 image (generated by the Generator2) and verify if it is real\n",
    "    4) <img src=\"hidden\\photo5.png\" alt=\"This image is a representation of the simple neural network\" style=\"width: 400px;\"/>  <br />  \n",
    "    5) <img src=\"hidden\\photo6.png\" alt=\"This image is a representation of the simple neural network\" style=\"width: 400px;\"/>  <br />  \n",
    "\n",
    "7) **Information of cycleGAN generator training:**\n",
    "    1) Consists of 4 losses to minimize during training (each loss is assigned with a weightage):\n",
    "        1) Adversarial loss as Mean Squared Error (MSE, also called L2): Involving data of output_d\n",
    "        2) Identity loss as Mean Absolute Error (MAE, also called L1): Involving data of output_id\n",
    "        3) Cycle loss forward as Mean Absolute Error (MAE, also called L1): Involving data of output_f\n",
    "        4) Cycle loss backward as Mean Absolute Error (MAE, also called L1): Involving data of output_b\n",
    "    2) <img src=\"hidden\\photo7.png\" alt=\"This image is a representation of the simple neural network\" style=\"width: 400px;\"/>  <br />  \n",
    "    3) <img src=\"hidden\\photo8.png\" alt=\"This image is a representation of the simple neural network\" style=\"width: 400px;\"/>  <br />\n",
    "        1) When we train a generator using the GAN model, the rest of generator and discriminators should be hold constant (set as not trainable)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import random\n",
    "from numpy import load\n",
    "from numpy import zeros\n",
    "from numpy import ones\n",
    "from numpy import asarray\n",
    "from numpy.random import randint\n",
    "from keras.optimizers import adam_v2\n",
    "from keras.initializers import RandomNormal\n",
    "from keras.models import Model\n",
    "from keras.models import Input\n",
    "from keras.layers import Conv2D\n",
    "from keras.layers import Conv2DTranspose\n",
    "from keras.layers import LeakyReLU\n",
    "from keras.layers import Activation\n",
    "from keras.layers import Concatenate\n",
    "#from keras_contrib.layers.normalization.instancenormalization import InstanceNormalization\n",
    "\n",
    "#Download instance norm. code from the link above.\n",
    "#Or install keras_contrib using guidelines here: https://github.com/keras-team/keras-contrib \n",
    "from instancenormalization import InstanceNormalization  \n",
    "\n",
    "from matplotlib import pyplot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the function to create/build/define and compile the discriminator network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# discriminator model (70x70 patchGAN). The only difference between the discriminator of the cycleGAN here and the one in pix2pix is the discriminator of the cycleGAN here uses the instance normalization.\n",
    "# C64-C128-C256-C512\n",
    "#After the last layer, conv to 1-dimensional output, followed by a Sigmoid function.  \n",
    "# The “axis” argument is set to -1 for instance norm. to ensure that features are normalized per feature map.\n",
    "def define_discriminator(image_shape):\n",
    "\t# weight initialization\n",
    "\tinit = RandomNormal(stddev=0.02)\n",
    "\t# source image input\n",
    "\tin_image = Input(shape=image_shape)\n",
    "\t# C64: 4x4 kernel Stride 2x2\n",
    "\td = Conv2D(64, (4,4), strides=(2,2), padding='same', kernel_initializer=init)(in_image)\n",
    "\td = LeakyReLU(alpha=0.2)(d)\n",
    "\t# C128: 4x4 kernel Stride 2x2\n",
    "\td = Conv2D(128, (4,4), strides=(2,2), padding='same', kernel_initializer=init)(d)\n",
    "\td = InstanceNormalization(axis=-1)(d)\n",
    "\td = LeakyReLU(alpha=0.2)(d)\n",
    "\t# C256: 4x4 kernel Stride 2x2\n",
    "\td = Conv2D(256, (4,4), strides=(2,2), padding='same', kernel_initializer=init)(d)\n",
    "\td = InstanceNormalization(axis=-1)(d)\n",
    "\td = LeakyReLU(alpha=0.2)(d)\n",
    "\t# C512: 4x4 kernel Stride 2x2 \n",
    "    # Not in the original paper. Comment this block if you want.\n",
    "\td = Conv2D(512, (4,4), strides=(2,2), padding='same', kernel_initializer=init)(d)\n",
    "\td = InstanceNormalization(axis=-1)(d)\n",
    "\td = LeakyReLU(alpha=0.2)(d)\n",
    "\t# second last output layer : 4x4 kernel but Stride 1x1\n",
    "\td = Conv2D(512, (4,4), padding='same', kernel_initializer=init)(d)\n",
    "\td = InstanceNormalization(axis=-1)(d)\n",
    "\td = LeakyReLU(alpha=0.2)(d)\n",
    "\t# patch output\n",
    "\tpatch_out = Conv2D(1, (4,4), padding='same', kernel_initializer=init)(d)\n",
    "\t# define model\n",
    "\tmodel = Model(in_image, patch_out)\n",
    "\t# compile model\n",
    "    #The model is trained with a batch size of one image and Adam opt. \n",
    "    #with a small learning rate and 0.5 beta. \n",
    "    #The loss for the discriminator is weighted by 50% for each model update.\n",
    "    #This slows down changes to the discriminator relative to the generator model during training.\n",
    "\tmodel.compile(loss='mse', optimizer=adam_v2.Adam(lr=0.0002, beta_1=0.5), loss_weights=[0.5])\n",
    "\treturn model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the function to create/build/define the resnet block (which will be used in the generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generator a resnet block to be used in the generator. The resnet block is all about taking the output from the convolution layers to concatenate with the corresponding input of the resnet block.\n",
    "# residual block that contains two 3 × 3 convolutional layers with the same number of filters on both layers.\n",
    "def resnet_block(n_filters, input_layer): \n",
    "\t# weight initialization\n",
    "\tinit = RandomNormal(stddev=0.02)\n",
    "\t# first convolutional layer\n",
    "\tg = Conv2D(n_filters, (3,3), padding='same', kernel_initializer=init)(input_layer)\n",
    "\tg = InstanceNormalization(axis=-1)(g)\n",
    "\tg = Activation('relu')(g)\n",
    "\t# second convolutional layer\n",
    "\tg = Conv2D(n_filters, (3,3), padding='same', kernel_initializer=init)(g)\n",
    "\tg = InstanceNormalization(axis=-1)(g)\n",
    "\t# concatenate merge channel-wise with input layer\n",
    "\tg = Concatenate()([g, input_layer])\n",
    "\treturn g"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the function to create/build/define the generator network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the  generator model - encoder-decoder type architecture\n",
    "\n",
    "#c7s1-k denote a 7×7 Convolution-InstanceNorm-ReLU layer with k filters and stride 1. \n",
    "#dk denotes a 3 × 3 Convolution-InstanceNorm-ReLU layer with k filters and stride 2.\n",
    "# Rk denotes a residual block that contains two 3 × 3 convolutional layers\n",
    "# uk denotes a 3 × 3 fractional-strided-Convolution InstanceNorm-ReLU layer with k filters and stride 1/2\n",
    "\n",
    "#The network with 6 residual blocks consists of:\n",
    "#c7s1-64,d128,d256,R256,R256,R256,R256,R256,R256,u128,u64,c7s1-3\n",
    "\n",
    "#The network with 9 residual blocks consists of:\n",
    "#c7s1-64,d128,d256,R256,R256,R256,R256,R256,R256,R256,R256,R256,u128, u64,c7s1-3\n",
    "\n",
    "def define_generator(image_shape, n_resnet=9):\n",
    "\t# weight initialization\n",
    "\tinit = RandomNormal(stddev=0.02)\n",
    "\t# image input\n",
    "\tin_image = Input(shape=image_shape)\n",
    "\t# c7s1-64\n",
    "\tg = Conv2D(64, (7,7), padding='same', kernel_initializer=init)(in_image)\n",
    "\tg = InstanceNormalization(axis=-1)(g)\n",
    "\tg = Activation('relu')(g)\n",
    "\t# d128\n",
    "\tg = Conv2D(128, (3,3), strides=(2,2), padding='same', kernel_initializer=init)(g)\n",
    "\tg = InstanceNormalization(axis=-1)(g)\n",
    "\tg = Activation('relu')(g)\n",
    "\t# d256\n",
    "\tg = Conv2D(256, (3,3), strides=(2,2), padding='same', kernel_initializer=init)(g)\n",
    "\tg = InstanceNormalization(axis=-1)(g)\n",
    "\tg = Activation('relu')(g)\n",
    "\t# R256\n",
    "\tfor _ in range(n_resnet):\n",
    "\t\tg = resnet_block(256, g)\n",
    "\t# u128\n",
    "\tg = Conv2DTranspose(128, (3,3), strides=(2,2), padding='same', kernel_initializer=init)(g)\n",
    "\tg = InstanceNormalization(axis=-1)(g)\n",
    "\tg = Activation('relu')(g)\n",
    "\t# u64\n",
    "\tg = Conv2DTranspose(64, (3,3), strides=(2,2), padding='same', kernel_initializer=init)(g)\n",
    "\tg = InstanceNormalization(axis=-1)(g)\n",
    "\tg = Activation('relu')(g)\n",
    "\t# c7s1-3\n",
    "\tg = Conv2D(3, (7,7), padding='same', kernel_initializer=init)(g)\n",
    "\tg = InstanceNormalization(axis=-1)(g)\n",
    "\tout_image = Activation('tanh')(g)\n",
    "\t# define model\n",
    "\tmodel = Model(in_image, out_image)\n",
    "\treturn model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the function to create/build/define and compile the GAN model (composite model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a composite model for updating generators by adversarial and cycle loss\n",
    "#We define a composite model that will be used to train each generator separately. \n",
    "def define_composite_model(g_model_1, d_model, g_model_2, image_shape): # g_model_1 and d_model is a pair (means d_model is the corresponding discriminator of the generator called g_model_1\n",
    "\t# Make the generator of interest trainable as we will be updating these weights.\n",
    "    #by keeping other models constant.\n",
    "    #Remember that we use this same function to train both generators,\n",
    "    #one generator at a time. \n",
    "\tg_model_1.trainable = True\n",
    "\t# mark the corresponding discriminator of the generator called g_model_1 and second generator as non-trainable\n",
    "\td_model.trainable = False\n",
    "\tg_model_2.trainable = False\n",
    "    \n",
    "\t# adversarial loss\n",
    "\tinput_gen = Input(shape=image_shape)\n",
    "\tgen1_out = g_model_1(input_gen)\n",
    "\toutput_d = d_model(gen1_out)\n",
    "\t# identity loss\n",
    "\tinput_id = Input(shape=image_shape)\n",
    "\toutput_id = g_model_1(input_id)\n",
    "\t# cycle loss - forward\n",
    "\toutput_f = g_model_2(gen1_out)\n",
    "\t# cycle loss - backward\n",
    "\tgen2_out = g_model_2(input_id)\n",
    "\toutput_b = g_model_1(gen2_out)\n",
    "    \n",
    "\t# define model graph\n",
    "\tmodel = Model([input_gen, input_id], [output_d, output_id, output_f, output_b])\n",
    "\t\n",
    "    # define the optimizer\n",
    "\topt = adam_v2.Adam(lr=0.0002, beta_1=0.5)\n",
    "\t# compile model with weighting of least squares loss and L1 loss\n",
    "\tmodel.compile(loss=['mse', 'mae', 'mae', 'mae'], \n",
    "               loss_weights=[1, 5, 10, 10], optimizer=opt)\n",
    "\treturn model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the function to load images (real images) from the dataset, then rescale the real images\n",
    "\n",
    "Since the generator network output layer uses tanh activation function (tanh activation function provides an output value in the range from -1 to 1), we convert(rescale) each feature (pixel value) of each real image (from dataset) into the range from -1 to 1 to match the output of generator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load and prepare training images\n",
    "def load_real_samples(filename):\n",
    "\t# load the dataset\n",
    "\tdata = load(filename)\n",
    "\t# unpack arrays\n",
    "\tX1, X2 = data['arr_0'], data['arr_1']\n",
    "\t# scale from [0,255] to [-1,1]\n",
    "\tX1 = (X1 - 127.5) / 127.5\n",
    "\tX2 = (X2 - 127.5) / 127.5\n",
    "\treturn [X1, X2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define a function to select a batch size of real samples/images (features) from the dataset, then assign each selected real samples/image a value of 1 as ground truth (target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select a batch of random samples, returns images and target\n",
    "#Remember that for real images the label (y) is 1. \n",
    "def generate_real_samples(dataset, n_samples, patch_shape):\n",
    "\t# choose random instances\n",
    "\tix = randint(0, dataset.shape[0], n_samples)\n",
    "\t# retrieve selected images\n",
    "\tX = dataset[ix]\n",
    "\t# generate 'real' class labels (1)\n",
    "\ty = ones((n_samples, patch_shape, patch_shape, 1))\n",
    "\treturn X, y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define a function to generate a batch size of fake samples/images (features) using the selected generator, then assign each selected fake sample/image a value of 0 as ground truth (target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate a batch of images, returns images and targets\n",
    "#Remember that for fake images the label (y) is 0. \n",
    "def generate_fake_samples(g_model, dataset, patch_shape):\n",
    "\t# generate fake images\n",
    "\tX = g_model.predict(dataset)\n",
    "\t# create 'fake' class labels (0)\n",
    "\ty = zeros((len(X), patch_shape, patch_shape, 1))\n",
    "\treturn X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define a function to save all generator models periodically"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# periodically save the generator models to file\n",
    "def save_models(step, g_model_AtoB, g_model_BtoA):\n",
    "\t# save the first generator model\n",
    "\tfilename1 = 'g_model_AtoB_%06d.h5' % (step+1)\n",
    "\tg_model_AtoB.save(filename1)\n",
    "\t# save the second generator model\n",
    "\tfilename2 = 'g_model_BtoA_%06d.h5' % (step+1)\n",
    "\tg_model_BtoA.save(filename2)\n",
    "\tprint('>Saved: %s and %s' % (filename1, filename2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define a function to generate images using the save model and plot input and output images periodically"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# periodically generate images using the save model and plot input and output images\n",
    "def summarize_performance(step, g_model, trainX, name, n_samples=5):\n",
    "\t# select a sample of input images\n",
    "\tX_in, _ = generate_real_samples(trainX, n_samples, 0)\n",
    "\t# generate translated images\n",
    "\tX_out, _ = generate_fake_samples(g_model, X_in, 0)\n",
    "\t# scale all pixels from [-1,1] to [0,1]\n",
    "\tX_in = (X_in + 1) / 2.0\n",
    "\tX_out = (X_out + 1) / 2.0\n",
    "\t# plot n_samples randomly selected real images and its corresponding fake images generated by the selected generator model as a 2x(n_samples) subplot/grid\n",
    "\t# plot real images at the 1st row of the subplot\n",
    "\tfor i in range(n_samples):\n",
    "\t\tpyplot.subplot(2, n_samples, 1 + i)\n",
    "\t\tpyplot.axis('off')\n",
    "\t\tpyplot.imshow(X_in[i])\n",
    "\t# plot translated image  at the 2nd row of the subplot\n",
    "\tfor i in range(n_samples):\n",
    "\t\tpyplot.subplot(2, n_samples, 1 + n_samples + i)\n",
    "\t\tpyplot.axis('off')\n",
    "\t\tpyplot.imshow(X_out[i])\n",
    "\t# save plot to file\n",
    "\tfilename1 = '%s_generated_plot_%06d.png' % (name, (step+1))\n",
    "\tpyplot.savefig(filename1)\n",
    "\tpyplot.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define a function to update image pool for fake images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# update image pool/buffer (that can store max_size images) for fake images to reduce model oscillation\n",
    "# update discriminators using a history of generated images \n",
    "#rather than the ones produced by the latest generators.\n",
    "#Original paper recommended keeping an image buffer that stores \n",
    "#the 50 previously created images.\n",
    "\n",
    "def update_image_pool(pool, images, max_size=50): # pools refers to a image pool of a particular domain that stores max_size numbers of previously generated fake images of that particular domain; images refers to a variable that stores currently generated fake images of a particular domain (same as the domain of the selected image pool);\n",
    "\tselected = list() # the variable selected will store the selected fake images (each fake image is added to the the variable selected either [directly take the current fake image provided in that iteration] or [take a previously generated fake image from the image pool, before replacing that taken fake image in the image pool with the current fake image provided in that iteration]) that eventually will be used for training. The number of fake images in the variable selected is same as the one of the variable images.\n",
    "\tfor image in images:\n",
    "\t\tif len(pool) < max_size: # when the image pool not yet full (still have room for fake images)\n",
    "\t\t\t# stock the pool\n",
    "\t\t\tpool.append(image) # add the current fake image to the image pool (the image pool is updated with a current fake image)\n",
    "\t\t\tselected.append(image) # add the current fake image to the variable selected\n",
    "\t\telif random() < 0.5: # When the image pool is full, and if the randomly generated number is < 0.5 (the probability of 50/50 will execute this command)\n",
    "\t\t\t# use image, but don't add it to the image pool\n",
    "\t\t\tselected.append(image) # add the current fake image to the variable selected\n",
    "\t\telse: # When the image pool is full, and if the randomly generated number is >= 0.5 (the probability of 50/50 will execute this command)\n",
    "\t\t\t# replace an existing image and use replaced image\n",
    "\t\t\tix = randint(0, len(pool)) # randomly select a previously generated fake image\n",
    "\t\t\tselected.append(pool[ix]) # add the selected previously generated fake image to the variable selected\n",
    "\t\t\tpool[ix] = image # replace the selected previously generated fake image with the current fake image (the image pool is updated with a current fake image)\n",
    "\treturn asarray(selected) # returns the selected fake images that eventually will be used for training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the function to perform the GAN model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train cyclegan models\n",
    "def train(d_model_A, d_model_B, g_model_AtoB, g_model_BtoA, c_model_AtoB, c_model_BtoA, dataset, epochs=1): # d_model refers to a discriminator, g_model refers to a generator, c_model refers to a composite mode (GAN model), A refers to the domain/type A, B refers to the domain/type B, AtoB meanns the model input is domain/type A and its output is domain/type B.\n",
    "\t# define properties of the training run\n",
    "\tn_epochs, n_batch, = epochs, 1  #batch size fixed to 1 as suggested in the paper\n",
    "\t# determine the output square shape of the discriminator\n",
    "\tn_patch = d_model_A.output_shape[1]\n",
    "\t# unpack dataset\n",
    "\ttrainA, trainB = dataset # trainA stores the real domain A images(samples); trainB stores the real domain B images(samples)\n",
    "\t# prepare image pool for fake images\n",
    "\tpoolA, poolB = list(), list() # poolA will be used to store max_size numbers of previously generated fake domain A images(samples); poolB will be used to store max_size numbers of previously generated fake domain B images(samples)\n",
    "\t# calculate the number of batches per training epoch (also same as the number of iterations per epoch)\n",
    "\tbat_per_epo = int(len(trainA) / n_batch)\n",
    "\t# calculate the number of training iterations\n",
    "\tn_steps = bat_per_epo * n_epochs\n",
    "    \n",
    "\t# manually enumerate epochs\n",
    "\tfor i in range(n_steps):\n",
    "\t\t# Part 1: select a batch of real samples from each domain (A and B)\n",
    "\t\tX_realA, y_realA = generate_real_samples(trainA, n_batch, n_patch) # X_realA stores a batch(group) of real domain A images(samples); y_realA stores a batch(group) of \"real image class\" target(ground truth) labels [values of 1] for each image(sample) in the X_realA.\n",
    "\t\tX_realB, y_realB = generate_real_samples(trainB, n_batch, n_patch) # X_realB stores a batch(group) of real domain B images(samples); y_realB stores a batch(group) of \"real image class\" target(ground truth) labels [values of 1] for each image(sample) in the X_realB.\n",
    "\t\t# Part 2: generate a batch of fake samples using both B to A and A to B generators.\n",
    "\t\tX_fakeA, y_fakeA = generate_fake_samples(g_model_BtoA, X_realB, n_patch) # X_fakeA stores a batch(group) of fake domain A images(samples) generated by the generator called g_model_BtoA; y_fakeA stores a batch(group) of \"fake image class\" target(ground truth) labels [values of 0] for each image(sample) in the X_fakeA.\n",
    "\t\tX_fakeB, y_fakeB = generate_fake_samples(g_model_AtoB, X_realA, n_patch) # X_fakeB stores a batch(group) of fake domain B images(samples) generated by the generator called g_model_AtoB; y_fakeB stores a batch(group) of \"fake image class\" target(ground truth) labels [values of 0] for each image(sample) in the X_fakeB.\n",
    "\t\t# Part 3: update fake images in the pool. Remember that the paper suggstes a buffer of 50 images (max_size=50)\n",
    "\t\tX_fakeA = update_image_pool(poolA, X_fakeA) # X_fakeA stores a batch(group) of the selected fake domain A images (a combination of currently generated fake domain A images in this epoch and previously generated fake domain A images taken from poolA) that eventually will be used for training\n",
    "\t\tX_fakeB = update_image_pool(poolB, X_fakeB) # X_fakeB stores a batch(group) of the selected fake domain B images (a combination of currently generated fake domain B images in this epoch and previously generated fake domain B images taken from poolB) that eventually will be used for training\n",
    "        \n",
    "\t\t# Part 4: update (means train the model and get its loss) generator B->A (g_model_BtoA) via the composite model (c_model_BtoA)\n",
    "\t\tg_loss2, _, _, _, _  = c_model_BtoA.train_on_batch([X_realB, X_realA], [y_realA, X_realA, X_realB, X_realA]) # g_loss2 refers to the loss the discriminator A classifies the fake domain A images as the real domain A images (adversarial loss)\n",
    "\t\t# Train the generator model called g_model_A by calling the composite model called c_model_BtoA, using 2 inputs (variable X_realB and X_realA). \n",
    "\t\t# Each sample in the variable X_realB will be provided to the composite model input layer called input_gen, while each sample in the variable X_realA will be provided to the composite model input layer called input_id.\n",
    "\t\t# Each sample in the variable y_realA is compared with each sample at the composite model output layer called output_d, to calculate the adversarial loss as MSE (which is returned to the variable g_loss2 here). Then, the parameters (weightages and biases) of the generator (g_model_BtoA) is updated. This loss has a weightage of 1.\n",
    "\t\t# Each sample in the variable X_realA is compared with each sample at the composite model output layer called output_id, to calculate the identity loss as MAE/L1 (which is not returned here, due to the second '_'). This loss has a weightage of 5.\n",
    "\t\t# Each sample in the variable X_realB is compared with each sample at the composite model output layer called output_f, to calculate the forward cycle loss as MAE/L1 (which is not returned here, due to the third '_'). This loss has a weightage of 10.\n",
    "\t\t# Each sample in the variable X_realA is compared with each sample at the composite model output layer called output_b, to calculate the backward cycle loss as MAE/L1 (which is not returned here, due to the forth '_'). This loss has a weightage of 10.\n",
    "\n",
    "\n",
    "\t\t# Part 5: update discriminator for A (d_model_A) -> [real/fake]\n",
    "\t\tdA_loss1 = d_model_A.train_on_batch(X_realA, y_realA) # dA_loss1 refers to the loss the discriminator A classifies the real domain A images as the real domain A images\n",
    "\t\t# Train the discriminator model called d_model_A by using 1 input (variable X_realA, the real domain A images), then update the parameters (weights and biases) of the discriminator model (d_model_A).\n",
    "\t\t# This means each sample in the variable X_realA will be provided to the discriminator model input layer called in_image. \n",
    "\t\t# Each sample in the variable y_realA is compared with each sample at the discriminator model output layer called patch_out, to calculate the loss as MSE (which is returned to the variable dA_loss1 here). Then, the parameters (weightages and biases) of the discriminator (d_model_A) is updated. This loss has a weightage of 0.5.\n",
    "\n",
    "\t\tdA_loss2 = d_model_A.train_on_batch(X_fakeA, y_fakeA) # dA_loss2 refers to the loss the discriminator A classifies the fake domain A images as the real domain A images\n",
    "\t\t# Train the discriminator model called d_model_A by using 1 input (variable X_fakeA, the fake domain A images), then update the parameters (weights and biases) of the discriminator model (d_model_A).\n",
    "\t\t# This means each sample in the variable X_fakeA will be provided to the discriminator model input layer called in_image.\n",
    "\t\t# Each sample in the variable y_fakeA is compared with each sample at the discriminator model output layer called patch_out, to calculate the loss as MSE (which is returned to the variable dA_loss2 here). Then, the parameters (weightages and biases) of the discriminator (d_model_A) is updated. This loss has a weightage of 0.5.\n",
    "\n",
    "        \n",
    "\t\t# Part 6: update generator A->B (g_model_AtoB) via the composite model (c_model_AtoB)\n",
    "\t\tg_loss1, _, _, _, _ = c_model_AtoB.train_on_batch([X_realA, X_realB], [y_realB, X_realB, X_realA, X_realB])  # g_loss1 refers to the loss the discriminator B classifies the fake domain B images as the real domain B images (adversarial loss)\n",
    "\t\t# Train the generator model called g_model_AtoB by calling the composite model called c_model_AtoB, using 2 inputs (variable X_realA and X_realB). \n",
    "\t\t# Each sample in the variable X_realA will be provided to the composite model input layer called input_gen, while each sample in the variable X_realB will be provided to the composite model input layer called input_id. \n",
    "\t\t# Each sample in the variable y_realB is compared with each sample at the composite model output layer called output_d, to calculate the adversarial loss as MSE (which is returned to the variable g_loss1 here). Then, the parameters (weightages and biases) of the generator (g_model_AtoB) is updated. This loss has a weightage of 1.\n",
    "\t\t# Each sample in the variable X_realB is compared with each sample at the composite model output layer called output_id, to calculate the identity loss as MAE/L1 (which is not returned here, due to the second '_'). This loss has a weightage of 5.\n",
    "\t\t# Each sample in the variable X_realA is compared with each sample at the composite model output layer called output_f, to calculate the forward cycle loss as MAE/L1 (which is not returned here, due to the third '_'). This loss has a weightage of 10.\n",
    "\t\t# Each sample in the variable X_realB is compared with each sample at the composite model output layer called output_b, to calculate the backward cycle loss as MAE/L1 (which is not returned here, due to the forth '_'). This loss has a weightage of 10.\n",
    "\n",
    "\n",
    "\t\t# Part 7: update discriminator for B (d_model_B) -> [real/fake]\n",
    "\t\tdB_loss1 = d_model_B.train_on_batch(X_realB, y_realB) # dB_loss1 refers to the loss the discriminator B classifies the real domain B images as the real domain B images\n",
    "\t\t# Train the discriminator model called d_model_B by using 1 input (variable X_realB, the real domain B images), then update the parameters (weights and biases) of the discriminator model (d_model_B).\n",
    "\t\t# This means each sample in the variable X_realB will be provided to the discriminator model input layer called in_image. \n",
    "\t\t# Each sample in the variable y_realB is compared with each sample at the discriminator model output layer called patch_out, to calculate the loss as MSE (which is returned to the variable dB_loss1 here). Then, the parameters (weightages and biases) of the discriminator (d_model_B) is updated. This loss has a weightage of 0.5.\n",
    "\n",
    "\t\tdB_loss2 = d_model_B.train_on_batch(X_fakeB, y_fakeB) # dB_loss2 refers to the loss the discriminator B classifies the fake domain B images as the real domain B images\n",
    "\t\t# Train the discriminator model called d_model_B by using 1 input (variable X_fakeB, the fake domain B images), then update the parameters (weights and biases) of the discriminator model (d_model_B).\n",
    "\t\t# This means each sample in the variable X_fakeB will be provided to the discriminator model input layer called in_image. \n",
    "\t\t# Each sample in the variable y_fakeB is compared with each sample at the discriminator model output layer called patch_out, to calculate the loss as MSE (which is returned to the variable dB_loss2 here). Then, the parameters (weightages and biases) of the discriminator (d_model_B) is updated. This loss has a weightage of 0.5.\n",
    "\n",
    "\n",
    "        # Part 8: summarize performance\n",
    "        # Since our batch size =1, the number of iterations would be same as the size of our dataset.\n",
    "        # In one epoch you'd have iterations equal to the number of images.\n",
    "        # If you have 100 images then 1 epoch would be 100 iterations\n",
    "\t\tprint('Iteration>%d, dA[%.3f,%.3f] dB[%.3f,%.3f] g[%.3f,%.3f]' % (i+1, dA_loss1,dA_loss2, dB_loss1,dB_loss2, g_loss1,g_loss2))\n",
    "\t\t\t# g_loss2 refers to the loss the discriminator A classifies the fake domain A images as the real domain A images (adversarial loss)\n",
    "\t\t\t# dA_loss1 refers to the loss the discriminator A classifies the real domain A images as the real domain A images\n",
    "\t\t\t# dA_loss2 refers to the loss the discriminator A classifies the fake domain A images as the real domain A images\n",
    "\t\t\t# g_loss1 refers to the loss the discriminator B classifies the fake domain B images as the real domain B images (adversarial loss)\n",
    "\t\t\t# dB_loss1 refers to the loss the discriminator B classifies the real domain B images as the real domain B images\n",
    "\t\t\t# dB_loss2 refers to the loss the discriminator B classifies the fake domain B images as the real domain B images\n",
    "\t\t\n",
    "\t\t# Evaluate the model performance periodically\n",
    "        # If batch size (total images)=100, performance will be summarized after every 75th iteration.\n",
    "\t\tif (i+1) % (bat_per_epo * 1) == 0:\n",
    "\t\t\t# plot A->B translation\n",
    "\t\t\tsummarize_performance(i, g_model_AtoB, trainA, 'AtoB') # Plot the subplot to show the performance of the trained g_model_AtoB (take a real domain A image to generate a fake domain B image)\n",
    "\t\t\t# plot B->A translation\n",
    "\t\t\tsummarize_performance(i, g_model_BtoA, trainB, 'BtoA') # Plot the subplot to show the performance of the trained g_model_AtoB (take a real domain B image to generate a fake domain A image) \n",
    "\t\tif (i+1) % (bat_per_epo * 5) == 0:\n",
    "\t\t\t# Save all the trained generator models\n",
    "            # If batch size (total images)=100, model will be saved after every 75th iteration x 5 = 375 iterations.\n",
    "\t\t\tsave_models(i, g_model_AtoB, g_model_BtoA) # Save all of the trained generator models called g_model_AtoB and g_model_BtoA respectively. So later you can deploy the correct trained generator model to either (taking a real domain A image to generate a fake domain B image) or (taking a real domain B image to generate a fake domain A image). \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GPUEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
