# References: https://stackoverflow.com/questions/74580942/the-shuffling-order-of-dataloader-in-pytorch

# Condition 1: num_worker=0 (Multiprocessing not enabled)
import torch
from torch.utils.data import DataLoader

datasets = [0,1,2,3,4]

torch.manual_seed(1)
G = torch.Generator()
G.manual_seed(1)

# this is different from OP's scenario II because in that case the ran_sampler is not initialized with the right generator.
dataloader = DataLoader(dataset=datasets, shuffle=True, generator=G) # Dataloader automatically creates a RandomSampler object with the generator and assign the same generator to self.generator. So id(dataloader.generator)==id(dataloader.sampler.generator) will return TRUE, and the sequence/series of shuffled indices used in each epoch will be different but are reproducible/deterministic
print(id(dataloader.generator)==id(dataloader.sampler.generator))
xs = []

for i in range(3):
    for x in dataloader:
        xs.append(x.item())
    
print(xs)

# # Condition 2: num_worker>0 (Multiprocessing enabled), the sequence/series of shuffled indices used in each epoch will be different but are reproducible/deterministic
# torch.manual_seed(1) # Initialize/seed the PyTorch random number generator (RNG) state, so that the PyTorch RNG will generate the reproducible/deterministic base seed whenever the dataloader object is called (If the PyTorch RNG is initialized/seeded only once before the dataloader object is called the first time, it will generate different base seeds at each time the dataloader object is called, but the base seeds are reproducible/deterministic. [EG: If the PyTorch RNG is initialized/seeded only once before the dataloader object is called the first time (before the first epoch), the sequence of shuffled indices used in each epoch will be different but are reproducible/deterministic. But if the PyTorch RNG is seeded/initialized everytime before the dataloader object is called (at the beginning of each epoch), the sequence of shuffled indices used in each epoch will be the same and are reproducible/determistic.]). Thus, the sequence of shuffled indices used in each epoch are reproducible/determistic at every time runing the script. This is because for the case of multiprocessing/multiworkers (such that num_workers>0), each worker will have its PyTorch seed set to base_seed + worker_id, where base_seed is generated by PyTorch RNG based on its state (its state will change everytime it is called, but its state can be initialized with torch.manual_seed()) while each worker has unique fixed worker id. 
# test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=config.test_batch_size, shuffle=True, num_workers=config.num_workers, pin_memory=True) # verified that the indices used in each epoch are reproducible at every time runing the script
# for i in range(3):
#     print(f"\nEpoch {i}:")
#     # torch.manual_seed(1) # the PyTorch RNG is seeded/initialized everytime before the dataloader object is called (at the beginning of each epoch)
#     for iteration, (indices, img_lowlight) in enumerate(test_loader):
#         print(f"Batch {iteration}, Indices: {indices}")


# # Condition 3: num_worker>0 (Multiprocessing enabled), the sequence/series of shuffled indices used in each epoch will be the same and are reproducible/determistic.
# test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=config.test_batch_size, shuffle=True, num_workers=config.num_workers, pin_memory=True) # verified that the indices used in each epoch are reproducible at every time runing the script
# for i in range(3):
#     print(f"\nEpoch {i}:")
#     torch.manual_seed(1) # the PyTorch RNG is seeded/initialized everytime before the dataloader object is called (at the beginning of each epoch). # Initialize/seed the PyTorch random number generator (RNG) state, so that the PyTorch RNG will generate the reproducible/deterministic base seed whenever the dataloader object is called (If the PyTorch RNG is initialized/seeded only once before the dataloader object is called the first time, it will generate different base seeds at each time the dataloader object is called, but the base seeds are reproducible/deterministic. [EG: If the PyTorch RNG is initialized/seeded only once before the dataloader object is called the first time (before the first epoch), the sequence of shuffled indices used in each epoch will be different but are reproducible/deterministic. But if the PyTorch RNG is seeded/initialized everytime before the dataloader object is called (at the beginning of each epoch), the sequence of shuffled indices used in each epoch will be the same and are reproducible/determistic.]). Thus, the sequence of shuffled indices used in each epoch are reproducible/determistic at every time runing the script. This is because for the case of multiprocessing/multiworkers (such that num_workers>0), each worker will have its PyTorch seed set to base_seed + worker_id, where base_seed is generated by PyTorch RNG based on its state (its state will change everytime it is called, but its state can be initialized with torch.manual_seed()) while each worker has unique fixed worker id. 
#     for iteration, (indices, img_lowlight) in enumerate(test_loader):
#         print(f"Batch {iteration}, Indices: {indices}")
